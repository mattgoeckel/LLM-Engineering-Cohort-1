{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "collapsed_sections": [
    "eo_QP1ITFfX2"
   ],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Unsupervised Pre-Training of GPT-Style Model\n",
    "\n",
    "In today's notebook, we'll be working through an example of how to do unsupervised pre-training of a GPT-style model.\n",
    "\n",
    "The base model we'll use is Andrej Karpathy's [nanoGPT](https://github.com/karpathy/nanoGPT).\n",
    "\n",
    "All of the model code can be found in the [`model.py`](https://github.com/karpathy/nanoGPT/blob/master/model.py) file!\n",
    "\n",
    "> NOTE: We will not be leveraging the parallized training strategy in this notebook - you can find all the required code in the provided repository."
   ],
   "metadata": {
    "id": "UWiGVj6njoDn"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Selection\n",
    "\n",
    "For the notebook today, we'll be using a toy dataset called `tinyshakespeare`. Feel free to use your own corpus here, just make sure it's contained within a single `.txt` file.\n",
    "\n",
    "You could extend this example to use the [OpenWebText](https://skylion007.github.io/OpenWebTextCorpus/) dataset, which was used to pre-train GPT-2.\n",
    "\n",
    "> NOTE: Training LLMs can take a very long time - in order to get results similar to the [GPT-2 paper](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) you will need 8xA100s and train for ~4-5 days using a pararellized strategy (DDP) on the OpenWebText Corpus.\n",
    "\n",
    "Let's start by grabbing our source repository for the day!"
   ],
   "metadata": {
    "id": "eHi04aEnkKEZ"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lMRsEQZy6tgc",
    "outputId": "98bccebc-829f-49df-974a-6b7bd4eb088f"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "fatal: destination path 'nanoGPT' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/karpathy/nanoGPT.git"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, we'll need to grab some dependencies.\n",
    "\n",
    "`cohere` and `openai` are recent dependencies of `tiktoken`, but we will not be leveraging them today."
   ],
   "metadata": {
    "id": "6l4CqoEDl7ks"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install tiktoken requests cohere openai -qU"
   ],
   "metadata": {
    "id": "d_gepPv1Qdj_"
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "First things first - let's download our dataset!\n",
    "\n",
    "We'll leverage the `requests` library to do this - and then we will split our resultant data into a `train` and `val` set. We want ~90% of our data to be training, and ~10% to be validation."
   ],
   "metadata": {
    "id": "70hSjXmZmCt3"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import requests\n",
    "import tiktoken\n",
    "import numpy as np\n",
    "\n",
    "current_path = \"/data/shakespeare\"\n",
    "data_url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
    "\n",
    "if not os.path.exists(current_path):\n",
    "    os.makedirs(current_path)\n",
    "\n",
    "# download the tiny shakespeare dataset\n",
    "input_file_path = os.path.join(os.path.dirname(current_path), 'input.txt')\n",
    "if not os.path.exists(input_file_path):\n",
    "\n",
    "    with open(input_file_path, 'w') as f:\n",
    "        f.write(requests.get(data_url).text)\n",
    "\n",
    "with open(input_file_path, 'r') as f:\n",
    "    data = f.read()\n",
    "print(input_file_path)\n",
    "n = len(data)\n",
    "train_data = data[:int(n*0.9)]### YOUR CODE HERE\n",
    "val_data = data[int(n*0.9):]### YOUR CODE HERE\n"
   ],
   "metadata": {
    "id": "T7qRWArUNiZ5",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "0db89ffd-d29e-43f2-9d72-ad102c35aa9e"
   },
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/data/input.txt\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let's get our `tokenizers` dependency so we can train a tokenizer on our data."
   ],
   "metadata": {
    "id": "wU9BG2CymU-a"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install tokenizers -qU"
   ],
   "metadata": {
    "id": "gFnrwKpQPsYh"
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will be training a \"byte-pair-encoding\" or \"BPE\" tokenizer. If you'd like to read more, you can find it [here](https://en.wikipedia.org/wiki/Byte_pair_encoding).\n",
    "\n",
    "Let's work through an example of what Byte-Pair Encoding (BPE) is doing, exactly, from this wonderful example provided by [Hugging Face](https://huggingface.co/docs/transformers/main/tokenizer_summary#byte-pair-encoding-bpe).\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "rmWXE5ctma9Z"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### What is BPE?\n",
    "\n",
    "First, we need to do a step called \"pre-tokenization\", which is - as it sounds - a tokenization step that occurs before we tokenize.\n",
    "\n",
    "The essential idea of BPE is that we need to understand common words and \"byte-pairs\" in them. So, in order to find \"common words\" we first need to find...words!\n",
    "\n",
    "Let's take the following text and break it apart into its word components.\n",
    "\n",
    "\n",
    "```\n",
    "After pre-tokenization, a set of unique words has been created and the frequency with which each word occurred in the training data has been determined. Next, BPE creates a base vocabulary consisting of all symbols that occur in the set of unique words and learns merge rules to form a new symbol from two symbols of the base vocabulary. It does so until the vocabulary has attained the desired vocabulary size. Note that the desired vocabulary size is a hyperparameter to define before training the tokenizer.\n",
    "```\n",
    "\n",
    "A naive way to do this would just be by splitting on spaces...and that is indeed what technique was used in GPT-2."
   ],
   "metadata": {
    "id": "GLecDiHbogvX"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "input_text = \"\"\"\n",
    "After pre-tokenization, a set of unique words has been created and the frequency with which each word occurred in the training data has been determined. Next, BPE creates a base vocabulary consisting of all symbols that occur in the set of unique words and learns merge rules to form a new symbol from two symbols of the base vocabulary. It does so until the vocabulary has attained the desired vocabulary size. Note that the desired vocabulary size is a hyperparameter to define before training the tokenizer.\n",
    "\"\"\"\n",
    "\n",
    "naive_word_list = input_text.split()"
   ],
   "metadata": {
    "id": "m34NDAGCpiz6"
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we can count our words and get their frequency."
   ],
   "metadata": {
    "id": "hR8k-2bopqjy"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "vocab_and_frequencies = defaultdict(int)\n",
    "\n",
    "for word in naive_word_list:\n",
    "  vocab_and_frequencies[\" \".join(list(word))] += 1\n",
    "\n",
    "sorted(vocab_and_frequencies.items(), key = lambda x: x[1], reverse=True)[:5]"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B_201bSQpvqD",
    "outputId": "49ec0a10-fc70-49f2-f534-5c6b720f1807"
   },
   "execution_count": 6,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('t h e', 8), ('a', 4), ('o f', 4), ('v o c a b u l a r y', 4), ('h a s', 3)]"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's find our \"base vocabulary\", which is going to be each symbol present in our original dataset."
   ],
   "metadata": {
    "id": "NckufSxxp-w5"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from typing import Dict, Tuple, List, Set\n",
    "\n",
    "def find_vocabulary_size(current_vocab: Dict[str, int]) -> int:\n",
    "  vocab = set()\n",
    "\n",
    "  for word in current_vocab.keys():\n",
    "    for subword in word.split():\n",
    "      vocab.add(subword)\n",
    "\n",
    "  return len(vocab)"
   ],
   "metadata": {
    "id": "BNcjzjDvvKjp"
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "find_vocabulary_size(vocab_and_frequencies)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pf3kCf-WvdBL",
    "outputId": "ded63de2-97d0-4e0e-cb01-b022991417cb"
   },
   "execution_count": 8,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "As we can see, there are 34 symbols in our base vocabulary. Let's convert our data into a form where we can capture each symbol separately."
   ],
   "metadata": {
    "id": "VoMq7GhKqf7p"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we can start constructing our pairs. We will look at all the pairs of symbols as they appear and take into consideration their frequency in our corpus."
   ],
   "metadata": {
    "id": "OGxrHYmftDTr"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def find_pairs_and_frequencies(current_vocab: Dict[str, int]) -> Dict[str, int]:\n",
    "  pairs = {}\n",
    "\n",
    "  for word, frequency in current_vocab.items():\n",
    "    symbols = word.split()\n",
    "\n",
    "    for i in range(len(symbols) - 1):\n",
    "      pair = (symbols[i], symbols[i + 1])\n",
    "      current_frequency = pairs.get(pair, 0)\n",
    "      pairs[pair] = current_frequency + frequency\n",
    "\n",
    "  return pairs"
   ],
   "metadata": {
    "id": "sTwvfTAErQN7"
   },
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "pairs_and_frequencies = find_pairs_and_frequencies(vocab_and_frequencies)"
   ],
   "metadata": {
    "id": "FudOaKmYv9-y"
   },
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "sorted(pairs_and_frequencies.items(), key = lambda x: x[1], reverse=True)[:5]"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oGIJfkk7wFYw",
    "outputId": "be4f5666-4f7f-45a8-9622-55e56063855d"
   },
   "execution_count": 11,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[(('t', 'h'), 11),\n",
       " (('i', 'n'), 10),\n",
       " (('r', 'e'), 8),\n",
       " (('h', 'e'), 8),\n",
       " (('a', 't'), 7)]"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that we have the frequent pairs - we can merge those pairs into a single token.\n",
    "\n",
    "Let's see how this process looks in code."
   ],
   "metadata": {
    "id": "OqORqdzwsZ6s"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import re\n",
    "\n",
    "def merge_vocab(most_common_pair: Tuple[str], current_vocab: Dict[str, int]) -> Dict[str, int]:\n",
    "  vocab_out = {}\n",
    "\n",
    "  pattern = re.escape(' '.join(most_common_pair))\n",
    "  replacement = ''.join(most_common_pair)\n",
    "\n",
    "  for word_in in current_vocab:\n",
    "      word_out = re.sub(pattern, replacement, word_in)\n",
    "      vocab_out[word_out] = current_vocab[word_in]\n",
    "\n",
    "  return vocab_out"
   ],
   "metadata": {
    "id": "L7ohHm2kshoY"
   },
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "new_vocab_and_frequencies = merge_vocab(\n",
    "    sorted(pairs_and_frequencies.items(), key = lambda x: x[1], reverse=True)[0][0],\n",
    "    vocab_and_frequencies\n",
    ")"
   ],
   "metadata": {
    "id": "Ab760KKuwzZ6"
   },
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "sorted(new_vocab_and_frequencies.items(), key = lambda x: x[1], reverse=True)[:5]"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L0XtvLbpxbSx",
    "outputId": "3b9e1234-1a8a-4873-a0c1-f9f570737335"
   },
   "execution_count": 14,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('th e', 8), ('a', 4), ('o f', 4), ('v o c a b u l a r y', 4), ('h a s', 3)]"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "After one merge, we can see that `t h` has been converted to `th`!\n",
    "\n",
    "Let's see how that impacted our vocabulary."
   ],
   "metadata": {
    "id": "9DPkBzj2u-me"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "find_vocabulary_size(new_vocab_and_frequencies)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bO_xegCtxjQf",
    "outputId": "195a8d56-773a-488a-dc24-9db55f7ac8fa"
   },
   "execution_count": 15,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can see that our vocabulary has increased by 1 as we've added the `th` symbol to it!\n",
    "\n",
    "In essence, BPE will continue to do this process until your desired vocabulary size (a hyper-parameter) is met!"
   ],
   "metadata": {
    "id": "o3M13D60xzZi"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training Our Tokenizer\n",
    "\n",
    "Now that we have some background on how BBPE works, lets move on to training our tokenizer for our model!\n",
    "\n",
    "Let's walk through the steps we'll take:\n",
    "\n",
    "1. Initialize our `Tokenizer` with a `BPE` model. Be sure to include the `unk_token`.\n",
    "\n",
    "  - [`Tokenizer`](https://huggingface.co/docs/tokenizers/api/tokenizer#tokenizer)\n",
    "  - [`Models`](https://huggingface.co/docs/tokenizers/api/models#models)\n",
    "\n",
    "2. We'll include a normalizer, applied at the sequence level, and we'll use `NFD()` to do so. More reading on Unicode Normalization Forms [here](https://unicode.org/reports/tr15/#Normalization_Forms_Table).\n",
    "\n",
    "  - [`NFD()`](https://huggingface.co/docs/tokenizers/api/normalizers#tokenizers.normalizers.NFD)\n",
    "\n",
    "3. We'll also add our `ByteLevel()` pre-tokenizer, and our `ByteLevelDecoder()` decoder.\n",
    "\n",
    "  - [`ByteLevel()`](https://huggingface.co/docs/tokenizers/api/pre-tokenizers#tokenizers.pre_tokenizers.ByteLevel)\n",
    "  - [`ByteLevelDecoder()`](https://huggingface.co/docs/tokenizers/api/decoders#tokenizers.decoders.ByteLevel)"
   ],
   "metadata": {
    "id": "BePYCbHly02H"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.decoders import ByteLevel as ByteLevelDecoder\n",
    "from tokenizers.normalizers import NFD, Sequence\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import ByteLevel\n",
    "\n",
    "tokenizer = Tokenizer(BPE(unk_token=\"[unk]\"))\n",
    "tokenizer.normalizer = Sequence([\n",
    "    NFD()\n",
    "])\n",
    "tokenizer.pre_tokenizer = ByteLevel() ### YOUR CODE HERE\n",
    "tokenizer.decoder = ByteLevelDecoder() ### YOUR CODE HERE"
   ],
   "metadata": {
    "id": "OrztE09OPosB"
   },
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "We'll want to add some special tokens to our tokenizer to ensure in has access to common token patterns.\n",
    "\n",
    "Let's use the following:\n",
    "\n",
    "- `\"<s>\"`    : bos_token - beginning of sequence token\n",
    "- `\"</s>\"`   : eos_token - end of sequence token\n",
    "- `\"<pad>\"`  : padding_token - token used to pad sequences\n",
    "- `\"<unk>\"`  : unk_token - token used to represent unknown tokens.\n",
    "- `\"<mask>\"` : mask_token - token used to mask parts of our sequence\n",
    "\n",
    "We're also going to set a target vocabulary of 50,000 tokens."
   ],
   "metadata": {
    "id": "dDqkNNdM1KsD"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "trainer = BpeTrainer(\n",
    "    vocab_size= 50000,\n",
    "    show_progress=True,\n",
    "    special_tokens=[\n",
    "      \"[s]\", \"[/s]\", \"[pad]\", \"[unk]\", \"[mask]\"\n",
    "    ]\n",
    ")"
   ],
   "metadata": {
    "id": "x9iQVhN3P3RN"
   },
   "execution_count": 17,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Nothing left to do but point it at our data-source and let it train!\n",
    "\n",
    "We'll use the `.train()` method to accomplish this task.\n",
    "\n",
    "> NOTE: Pay attention to the desired inputs of the `.train()` method.\n",
    "\n",
    "- [`Tokenizer.train()`](https://huggingface.co/docs/tokenizers/api/tokenizer#tokenizers.Tokenizer.train)"
   ],
   "metadata": {
    "id": "yQ8X9vZe2Fyw"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "tokenizer.train(\n",
    "    files=[\"/data/input.txt\"],\n",
    "    trainer=trainer\n",
    ")"
   ],
   "metadata": {
    "id": "LinLHotSP7gv"
   },
   "execution_count": 18,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we can save our tokenizer - and then load it as a `GPT2Tokenizer` through the Hugging Face Library!"
   ],
   "metadata": {
    "id": "V2JNYiqB2qKV"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "save_path = '/content/tokenizer'\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "tokenizer.model.save(save_path)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jk6QjDGHQy2K",
    "outputId": "98a0f3bf-2b0d-40b2-98f3-7b3019d0d6ee"
   },
   "execution_count": 19,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['/content/tokenizer/vocab.json', '/content/tokenizer/merges.txt']"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install transformers -qU"
   ],
   "metadata": {
    "id": "cOOlbggdRFrN"
   },
   "execution_count": 20,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(save_path, unk_token=\"[UNK]\")"
   ],
   "metadata": {
    "id": "us1vofdhQ45C"
   },
   "execution_count": 21,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's see how it tokenizes our inputs!"
   ],
   "metadata": {
    "id": "0-Bnq7lV2xWo"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "input_sentence = \"Hark, my name be Romeo! I am but a beautiful summer's day!\""
   ],
   "metadata": {
    "id": "dnYnFa3fTRLf"
   },
   "execution_count": 22,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "tokenized_sentence = tokenizer.tokenize(input_sentence)\n",
    "tokenized_sentence"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PSHY5VufRbBj",
    "outputId": "09779fc1-e580-4412-9503-c980f92790f5"
   },
   "execution_count": 23,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['Hark',\n",
       " ',',\n",
       " 'Ġmy',\n",
       " 'Ġname',\n",
       " 'Ġbe',\n",
       " 'ĠRomeo',\n",
       " '!',\n",
       " 'ĠI',\n",
       " 'Ġam',\n",
       " 'Ġbut',\n",
       " 'Ġa',\n",
       " 'Ġbeautiful',\n",
       " 'Ġsummer',\n",
       " \"'s\",\n",
       " 'Ġday',\n",
       " '!']"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "encoded_tokens = tokenizer.encode(tokenized_sentence)\n",
    "encoded_tokens"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eZrWzQQlTU41",
    "outputId": "b4070ab0-239d-466d-d253-911f9cc2011a"
   },
   "execution_count": 24,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[12077, 9, 124, 637, 121, 826, 5, 87, 295, 219, 72, 9113, 2999, 141, 511, 5]"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "decoded_tokens = tokenizer.decode(encoded_tokens)\n",
    "decoded_tokens"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "oS6lE-NLRnzk",
    "outputId": "f32e7918-b017-4558-9281-7784816e7230"
   },
   "execution_count": 25,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"Hark, my name be Romeo! I am but a beautiful summer's day!\""
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      }
     },
     "metadata": {},
     "execution_count": 25
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tokenizing Dataset\n",
    "\n",
    "Now that we have trained our tokenizer - let's create a dataset we can leverage with the `nanoGPT` library.\n",
    "\n",
    "We'll simply encode our training and validation data - and then save them in binary files for later!\n",
    "\n",
    "> NOTE: Pay attention to the format you want your dataset in. We want ids, which means we want to use the [`.encode()`](https://huggingface.co/docs/tokenizers/api/tokenizer#tokenizers.Tokenizer.encode) method of our tokenizer."
   ],
   "metadata": {
    "id": "ji3sF-rA21YH"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "train_ids = tokenizer.encode(tokenizer.tokenize(train_data))\n",
    "val_ids = tokenizer.encode(tokenizer.tokenize(val_data))\n",
    "print(f\"train has {len(train_ids):,} tokens\")\n",
    "print(f\"val has {len(val_ids):,} tokens\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "calHML6JPnCU",
    "outputId": "c2656a92-105d-4e3c-ee78-2673aaa54e1a"
   },
   "execution_count": 26,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train has 291,284 tokens\n",
      "val has 34,223 tokens\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# export to bin files\n",
    "data_path = \"/data/shakespeare/\"\n",
    "\n",
    "train_ids = np.array(train_ids, dtype=np.uint16)\n",
    "val_ids = np.array(val_ids, dtype=np.uint16)\n",
    "train_ids.tofile(os.path.join(os.path.dirname(data_path), 'train.bin'))\n",
    "val_ids.tofile(os.path.join(os.path.dirname(data_path), 'val.bin'))"
   ],
   "metadata": {
    "id": "nKJ1KqiiPkRh"
   },
   "execution_count": 27,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training The Model\n",
    "\n",
    "Now that we have our tokenized dataset, let's get to training our model!\n",
    "\n",
    "We have a lot of set-up to do before we click \"`.train()`\", so let's jump right into it!\n",
    "\n",
    "First, let's literally jump into the `nanoGPT` repository we cloned earlier."
   ],
   "metadata": {
    "id": "c0I3VrRC3XIO"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%cd nanoGPT"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NUU2jaalUdqm",
    "outputId": "cedb9d76-d178-4635-b35b-07212fe9b632"
   },
   "execution_count": 28,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/content/nanoGPT\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "We'll do some critical imports."
   ],
   "metadata": {
    "id": "13p1e8sa3k0V"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "import pickle\n",
    "from contextlib import nullcontext\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# from the local repo\n",
    "from model import GPTConfig, GPT"
   ],
   "metadata": {
    "id": "weNR37BwUYNg"
   },
   "execution_count": 29,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Hyper-Parameters\n",
    "\n",
    "We have a laundry list of hyper-parameters to set up - let's walk through them and what they mean."
   ],
   "metadata": {
    "id": "kY_vWZG-3uM-"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### I/O\n",
    "\n",
    "- `out_dir` - simple enough, this is the output directory where our checkpoints are saved"
   ],
   "metadata": {
    "id": "OykCjVQK5EX-"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "out_dir = 'out'"
   ],
   "metadata": {
    "id": "viM3qlWt5PVS"
   },
   "execution_count": 30,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Initialization\n",
    "\n",
    "Since we're training from scratch, we'll use `init_from = 'scratch'`."
   ],
   "metadata": {
    "id": "A5iwwrNL5H4C"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "init_from = 'scratch'"
   ],
   "metadata": {
    "id": "OK1z2m3C312T"
   },
   "execution_count": 31,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Eval and Logging\n",
    "\n",
    "- `eval_interval` - this is the number of steps between evaluation stages, we'll want to see this ~`250`. Our model will be incredibly prone to over-fitting, and this will let us monitor with relative frequency.\n",
    "- `log_interval` - this is how often our training progress will log. You can set this ~`10`. It's dealer's choice, really.\n",
    "- `eval_iters` - this is how *many* iterations we want to evaluate for.\n",
    "- `eval_only` - this would evaluate our model - but not train it. We'll leave this as `False` for now.\n",
    "- `always_save_checkpoint` - this will always save our most recent checkpoint, regardless of metrics. For this example, we'll set this to `True`."
   ],
   "metadata": {
    "id": "2YlolKOj4_dE"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "eval_interval = 250\n",
    "eval_iters = 10\n",
    "log_interval = 8\n",
    "eval_only = False\n",
    "always_save_checkpoint = True"
   ],
   "metadata": {
    "id": "MbFN5Ltq4_mo"
   },
   "execution_count": 32,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Dataset\n",
    "\n",
    "We can set our dataset here - we'll use the one we created earlier!"
   ],
   "metadata": {
    "id": "a488zaF_4zQk"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "dataset = 'shakespeare'"
   ],
   "metadata": {
    "id": "_QC7vWXC40Hp"
   },
   "execution_count": 33,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Typical Hyper-Parameters\n",
    "\n",
    "- `gradient_accumulation_steps` - we can use gradient accumulation to \"simulate\" larger batch sizes by combining multiple different optimization steps together, without needing the additional memory for large batch sizes. We don't need to worry so much about this for the toy problem - but this hyper-parameter can be configured for larger training runs. [Here](https://lightning.ai/blog/gradient-accumulation/) is some great reading on the topic.\n",
    "- `batch_size` - Typical batch_size - the larger the merrier (up to a point) we'll be using `16` to ensure we do not exceed the memory quota of our GPU.\n",
    "- `block_size` - this can be thought of as another term for the `context window` of our model. Since our model cannot take variable length inputs - we use this to set all inputs to our desired size. We'll use a value of `512` to ensure speedy training."
   ],
   "metadata": {
    "id": "XP9rBgGc426Q"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "gradient_accumulation_steps = 8\n",
    "batch_size = 16\n",
    "block_size = 512"
   ],
   "metadata": {
    "id": "EM_ybLPP43Pd"
   },
   "execution_count": 34,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Model Architecture\n",
    "\n",
    "- `n_layer` - this is the number of decoder layers we will use in our model. More would be considered better (up to a point) and the original GPT-2 paper uses `12`, but we will be using a truncated `6` for ease and speed of training.\n",
    "- `n_head` - this is the number of attention heads in each decoder layer!\n",
    "- `n_embd` - this is the embedding dimension of our model, this is analagous to our `model_d` from the previous notebook. A default value of ~`500` should do the trick!\n",
    "- `dropout` - this sets our dropout value, since our model is small and going to be extremely prone to overfitting, consider setting this at a fairly aggresive level (`0.2` was used in the example training found in the notebook`).\n",
    "- `bias` - wether or not to use bias inside the LayerNorm/Linear layers."
   ],
   "metadata": {
    "id": "UZ-8bDIY45GS"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "n_layer = 4\n",
    "n_head = 4\n",
    "n_embd = 560\n",
    "dropout = 0.2\n",
    "bias = False\n",
    "\n",
    "'''\n",
    "I recieved the following error when I set the n_embd value to 512:\n",
    "RuntimeError: Expected is_sm80 || is_sm90 to be true, but got false.\n",
    "'''"
   ],
   "metadata": {
    "id": "gMyyDBxB6k4H",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "outputId": "9c9d632d-b954-44be-a297-8e4ab5233977"
   },
   "execution_count": 93,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\nI recieved the following error when I set the n_embd value to 512:\\n'"
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      }
     },
     "metadata": {},
     "execution_count": 93
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### ❓ QUESTION:\n",
    "\n",
    "What condition must be true as it relates to the `n_embd` and `n_head`?\n",
    "\n",
    "#### ANSWER:\n",
    "n_embd must be divisible by n_head"
   ],
   "metadata": {
    "id": "piNHkSaRNDjM"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Optimizer Hyper-Parameters\n",
    "\n",
    "Basic Optimizer Hyper-Parameters:\n",
    "\n",
    "- `learning_rate` - it's our learning rate! We'll want to set this fairly high ~`1e-3` since we're training on such a small dataset.\n",
    "- `max_iters` - how many iterations do we train for. More iters means longer training times. Feel free to tinker with this value! `5000` is a great place to start.\n",
    "\n",
    "Learning Rate Decay Settings:\n",
    "\n",
    "- `decay_lr` - set decay flag\n",
    "- `weight_Decay` - how much to decay lr by\n",
    "- `lr_decay_iters` - should be set to ~max_iters.\n",
    "- `min_lr` - the minimum lr, should be ~ lr / 10\n",
    "\n",
    "Clipping and Warmup:\n",
    "\n",
    "- `grad_clip` - value to clip gradients to. useful for preventing vanishing gradients.\n",
    "- `warmup_iters` - how many iterations to warmup for. Warmup is useful to allow your training to slowly warmup. It will use a low lr for a number of steps to avoid any massive initial spikes. Since we're training a very small model - we can avoid using many wamrup steps.\n",
    "\n",
    "> NOTE: Many learnings taken from the [Chincilla paper](https://arxiv.org/pdf/2203.15556.pdf) for selecting default or appropriate values."
   ],
   "metadata": {
    "id": "3NWDTaAz7gwh"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# adamw optimizer\n",
    "learning_rate = 1e-3\n",
    "max_iters = 5000\n",
    "beta1 = 0.9\n",
    "beta2 = 0.99\n",
    "\n",
    "# lr decay settings\n",
    "decay_lr = True\n",
    "weight_decay = 1e-1 ### YOUR CODE HERE\n",
    "lr_decay_iters = max_iters # ~= max_iters per Chinchilla\n",
    "min_lr = int(learning_rate/10) # ~= learning_rate/10 per Chinchilla\n",
    "\n",
    "# clipping and warmup\n",
    "grad_clip = 1.0\n",
    "warmup_iters = 100"
   ],
   "metadata": {
    "id": "qe-669jwUptI"
   },
   "execution_count": 71,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### ❓ QUESTION:\n",
    "\n",
    "Given a Learning Rate of `1e-4` and a maximum iteration cap of `10,000`: What should `lr_decay_iters` be, and what should `min_lr` be?\n",
    "\n",
    "#### ANSWER:\n",
    "\n",
    "lr_decay_iters should be 10,000 and min_lr should be 1e-5"
   ],
   "metadata": {
    "id": "AzHvpMDTNfU6"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "These hyper-parameters are necessary to set given the task we're training and given the environment we're training in."
   ],
   "metadata": {
    "id": "ucldc4mz9yeT"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "backend = 'nccl'\n",
    "device = 'cuda'\n",
    "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16'\n",
    "compile = True\n",
    "# -----------------------------------------------------------------------------\n",
    "config_keys = [k for k,v in globals().items() if not k.startswith('_') and isinstance(v, (int, float, bool, str))]\n",
    "config = {k: globals()[k] for k in config_keys}\n",
    "# -----------------------------------------------------------------------------\n",
    "master_process = True\n",
    "seed_offset = 0\n",
    "ddp_world_size = 1\n",
    "tokens_per_iter = gradient_accumulation_steps * ddp_world_size * batch_size * block_size\n",
    "print(f\"tokens per iteration will be: {tokens_per_iter:,}\")\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "print(out_dir)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xHiGlMOp8Nux",
    "outputId": "b2595475-ccd5-4128-bf53-d4fd5ba5756f"
   },
   "execution_count": 72,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tokens per iteration will be: 65,536\n",
      "out\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Torch Settings\n",
    "\n",
    "We need to set a few `torch` settings, including the seed, to allow us to train correctly on our GPU.\n",
    "\n",
    "Not much is required for us to understand here - these are just necessary lines of code. Boilerplate."
   ],
   "metadata": {
    "id": "eKmdfbye-BNf"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "torch.manual_seed(1337 + seed_offset)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
    "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu'\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)"
   ],
   "metadata": {
    "id": "yh34QGD6VARU"
   },
   "execution_count": 73,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Dataloader\n",
    "\n",
    "This block will:\n",
    "\n",
    "1. Set the data path\n",
    "2. Load the dataset we tokenized earlier from the `.bin` we saved\n",
    "3. Define a `get_batch` function that will return us a random section of our data as well as a the corresponding \"label\" for that data and move it to the GPU for easy use inside our training loop."
   ],
   "metadata": {
    "id": "gKeNwYaZ-Zoc"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "data_dir = os.path.join('/data', dataset)\n",
    "train_data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')\n",
    "val_data = np.memmap(os.path.join(data_dir, 'val.bin'), dtype=np.uint16, mode='r')\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
    "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
    "    if device_type == 'cuda':\n",
    "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
    "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
    "    else:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "get_batch('train')"
   ],
   "metadata": {
    "id": "tOjaPyJpVEgx",
    "is_executing": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### ❓ QUESTION:\n",
    "\n",
    "What can you tell us about the way the labels are generated?\n",
    "\n",
    "Please produce an example of a single x and y pair.\n",
    "\n",
    "#### ANSWER:\n",
    "\n",
    "The labels are randomly obtained using the for i in ix (ix being a tensor filled with random integers generated uniformly between low (inclusive) and high (exclusive)) for X. Y is all the data from X minus the first character plus the next character. This creates a shifting context window where the ML model is learning to solve for Y using X as the input. For example, if the pre-encoded train_data was \"Hello, my name is Slim Shady.\", then the x could be \"Hello, my name is Slim Shad\" and the y could be \"ello, my name is Slim Shady\". Here is a specific example from the Shakespeare dataset:\n",
    "\n",
    "### X\n",
    "First Citizen:\n",
    "Before we proceed any further, hear me speak.\n",
    "\n",
    "All:\n",
    "Speak, speak.\n",
    "\n",
    "First Citizen:\n",
    "You are all resolved rather to die than to famish?\n",
    "\n",
    "All:\n",
    "Resolved. resolved.\n",
    "\n",
    "First Citizen:\n",
    "First, you know Caius Marcius is chief enemy to the people.\n",
    "\n",
    "All:\n",
    "We know't, we know't.\n",
    "\n",
    "First Citizen:\n",
    "Let us kill him, and we'll have corn at our own price.\n",
    "Is't a verdict?\n",
    "\n",
    "All:\n",
    "No more talking on't; let it be done: away, away!\n",
    "\n",
    "Second Citizen:\n",
    "One word, good citizens.\n",
    "\n",
    "First Citizen:\n",
    "We are accounted poor citizens, the patricians good.\n",
    "What authority surfeits on would relieve us: if they\n",
    "would yield us but the superfluity, while it were\n",
    "wholesome, we might guess they relieved us humanely;\n",
    "but they think we are too dear: the leanness that\n",
    "afflicts us, the object of our misery, is as an\n",
    "inventory to particularise their abundance; our\n",
    "sufferance is a gain to them Let us revenge this with\n",
    "our pikes, ere we become rakes: for the gods know I\n",
    "speak this in hunger for bread, not in thirst for revenge.\n",
    "\n",
    "Second Citizen:\n",
    "Would you proceed especially against Caius Marcius?\n",
    "\n",
    "All:\n",
    "Against him first: he's a very dog to the commonalty.\n",
    "\n",
    "Second Citizen:\n",
    "Consider you what services he has done for his country?\n",
    "\n",
    "First Citizen:\n",
    "Very well; and could be content to give him good\n",
    "report fort, but that he pays himself with being proud.\n",
    "\n",
    "Second Citizen:\n",
    "Nay, but speak not maliciously.\n",
    "\n",
    "First Citizen:\n",
    "I say unto you, what he hath done famously, he did\n",
    "it to that end: though soft-conscienced men can be\n",
    "content to say it was for his country he did it to\n",
    "please his mother and to be partly proud; which he\n",
    "is, even till the altitude of his virtue.\n",
    "\n",
    "Second Citizen:\n",
    "What he cannot help in his nature, you account a\n",
    "vice in him. You must in no way say he is covetous.\n",
    "\n",
    "First Citizen:\n",
    "If I must not\n",
    "\n",
    "### Y\n",
    "irst Citizen:\n",
    "Before we proceed any further, hear me speak.\n",
    "\n",
    "All:\n",
    "Speak, speak.\n",
    "\n",
    "First Citizen:\n",
    "You are all resolved rather to die than to famish?\n",
    "\n",
    "All:\n",
    "Resolved. resolved.\n",
    "\n",
    "First Citizen:\n",
    "First, you know Caius Marcius is chief enemy to the people.\n",
    "\n",
    "All:\n",
    "We know't, we know't.\n",
    "\n",
    "First Citizen:\n",
    "Let us kill him, and we'll have corn at our own price.\n",
    "Is't a verdict?\n",
    "\n",
    "All:\n",
    "No more talking on't; let it be done: away, away!\n",
    "\n",
    "Second Citizen:\n",
    "One word, good citizens.\n",
    "\n",
    "First Citizen:\n",
    "We are accounted poor citizens, the patricians good.\n",
    "What authority surfeits on would relieve us: if they\n",
    "would yield us but the superfluity, while it were\n",
    "wholesome, we might guess they relieved us humanely;\n",
    "but they think we are too dear: the leanness that\n",
    "afflicts us, the object of our misery, is as an\n",
    "inventory to particularise their abundance; our\n",
    "sufferance is a gain to them Let us revenge this with\n",
    "our pikes, ere we become rakes: for the gods know I\n",
    "speak this in hunger for bread, not in thirst for revenge.\n",
    "\n",
    "Second Citizen:\n",
    "Would you proceed especially against Caius Marcius?\n",
    "\n",
    "All:\n",
    "Against him first: he's a very dog to the commonalty.\n",
    "\n",
    "Second Citizen:\n",
    "Consider you what services he has done for his country?\n",
    "\n",
    "First Citizen:\n",
    "Very well; and could be content to give him good\n",
    "report fort, but that he pays himself with being proud.\n",
    "\n",
    "Second Citizen:\n",
    "Nay, but speak not maliciously.\n",
    "\n",
    "First Citizen:\n",
    "I say unto you, what he hath done famously, he did\n",
    "it to that end: though soft-conscienced men can be\n",
    "content to say it was for his country he did it to\n",
    "please his mother and to be partly proud; which he\n",
    "is, even till the altitude of his virtue.\n",
    "\n",
    "Second Citizen:\n",
    "What he cannot help in his nature, you account a\n",
    "vice in him. You must in no way say he is covetous.\n",
    "\n",
    "First Citizen:\n",
    "If I must not,"
   ],
   "metadata": {
    "id": "I-tifZVD-9hG"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Simple Initialization of Model\n",
    "\n",
    "Here we init our number of iterations as 0, and our best val loss as a very high number."
   ],
   "metadata": {
    "id": "EbDlW-68_atH"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "iter_num = 0\n",
    "best_val_loss = 1e9"
   ],
   "metadata": {
    "id": "6hsepdVBVzQU"
   },
   "execution_count": 75,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Obtain our vocab size from our trained tokenizer."
   ],
   "metadata": {
    "id": "A4Uj9qBI_vXc"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "meta_path = os.path.join(data_dir, 'meta.pkl')\n",
    "meta_vocab_size = None\n",
    "if os.path.exists(meta_path):\n",
    "    with open(meta_path, 'rb') as f:\n",
    "        meta = pickle.load(f)\n",
    "    meta_vocab_size = meta['vocab_size']\n",
    "    print(f\"found vocab_size = {meta_vocab_size} (inside {meta_path})\")\n",
    "meta_vocab_size"
   ],
   "metadata": {
    "id": "m53DcCdFV0_a"
   },
   "execution_count": 76,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create our model args dict.\n",
    "\n",
    "Use the following as a guide: [Here](https://github.com/karpathy/nanoGPT/blob/eba36e84649f3c6d840a93092cb779a260544d08/model.py#L109)"
   ],
   "metadata": {
    "id": "V7bcNelYARmD"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "model_args = dict(\n",
    "    n_layer=n_layer, n_head=n_head, n_embd=n_embd, block_size=block_size,\n",
    "                  bias=bias, vocab_size=None, dropout=dropout\n",
    ")"
   ],
   "metadata": {
    "id": "JfIWEbanV7ZS"
   },
   "execution_count": 77,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Instantiate our model with the provided `model_args`.\n",
    "\n",
    "These are derived from the hyper-parameters we set above."
   ],
   "metadata": {
    "id": "2WWcbkiCAUI2"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "if init_from == 'scratch':\n",
    "    print(\"Initializing a new model from scratch\")\n",
    "    if meta_vocab_size is None:\n",
    "        print(\"defaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency)\")\n",
    "    model_args['vocab_size'] = meta_vocab_size if meta_vocab_size is not None else 50304\n",
    "    gptconf = GPTConfig(**model_args)\n",
    "    model = GPT(gptconf)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1Xly4iA0V-vF",
    "outputId": "b4a03d5a-ba4b-42e8-ad2a-0a708f023d3b"
   },
   "execution_count": 78,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Initializing a new model from scratch\n",
      "defaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency)\n",
      "number of parameters: 43.23M\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "There we go! If you used the default values - you should have a model with 29.55M parameters!\n",
    "\n",
    "Let's set our block_size to the correct size as determined in our configuration steps."
   ],
   "metadata": {
    "id": "BpViOsxLAl6p"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "if block_size < model.config.block_size:\n",
    "    model.crop_block_size(block_size)\n",
    "    model_args['block_size'] = block_size"
   ],
   "metadata": {
    "id": "TrEawNxdWRhm"
   },
   "execution_count": 79,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we can look at our model in all its glory!"
   ],
   "metadata": {
    "id": "eRgguPLKAuZ5"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "model.to(device)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zaE3KSTnAtJs",
    "outputId": "f1541f29-3e59-4a74-8d4a-86353876df1d"
   },
   "execution_count": 80,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(50304, 560)\n",
       "    (wpe): Embedding(512, 560)\n",
       "    (drop): Dropout(p=0.2, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-3): 4 x Block(\n",
       "        (ln_1): LayerNorm()\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=560, out_features=1680, bias=False)\n",
       "          (c_proj): Linear(in_features=560, out_features=560, bias=False)\n",
       "          (attn_dropout): Dropout(p=0.2, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=560, out_features=2240, bias=False)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=2240, out_features=560, bias=False)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=560, out_features=50304, bias=False)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 80
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "We'll set up our GradScaler - more information on this process [here](https://pytorch.org/docs/stable/amp.html#gradient-scaling)."
   ],
   "metadata": {
    "id": "LzoEY6gcBOSp"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))"
   ],
   "metadata": {
    "id": "BNUThRt4WT5H"
   },
   "execution_count": 81,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's set up our optimizer below. Be sure to include the correct values. You can check the `model.py` file for more information on what is expected in the `configure_optimizers` method [here](https://github.com/karpathy/nanoGPT/blob/eba36e84649f3c6d840a93092cb779a260544d08/model.py#L263C85-L263C85)."
   ],
   "metadata": {
    "id": "6Zs5Hcf9BBUD"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "optimizer = model.configure_optimizers(\n",
    "    weight_decay,\n",
    "    learning_rate,\n",
    "    (beta1, beta2),\n",
    "    device_type\n",
    ")\n",
    "\n",
    "checkpoint = None"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YesGeUnoWViL",
    "outputId": "dc2f2ce7-ad40-4862-80eb-7d2d32a515a7"
   },
   "execution_count": 82,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "num decayed parameter tensors: 18, with 43,509,760 parameters\n",
      "num non-decayed parameter tensors: 9, with 5,040 parameters\n",
      "using fused AdamW: True\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we can compile our model!\n",
    "\n",
    "If you're using the T4 or V100 instance of Colab - this will not provide a signficant speed-up, but if you're using Ampere architecture (A100) you should notice a significant difference between the compiled and uncompiled model.\n",
    "\n",
    "Read more about `torch.compile()` [here](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html)."
   ],
   "metadata": {
    "id": "ZF5YWJoKB4og"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "if compile:\n",
    "    print(\"compiling the model... (takes a ~minute)\")\n",
    "    unoptimized_model = model\n",
    "    model = torch.compile(model) # requires PyTorch 2.0"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v0FNU0T0WXdI",
    "outputId": "57bf77fa-9b6f-418d-a71b-95197e47b6f5"
   },
   "execution_count": 83,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "compiling the model... (takes a ~minute)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "We'll set up our loss estimation function here, which will help us estimate an arbitrarily accurate loss over either training or validation data by using many batches.\n",
    "\n",
    "You'll notice that we quickly convert the model into `.eval()` model and then back to `.train()` mode."
   ],
   "metadata": {
    "id": "p6lRcVsZCXRO"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            with ctx:\n",
    "                logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ],
   "metadata": {
    "id": "lUB5zVLVWbhM"
   },
   "execution_count": 84,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Creating our LR Scheduler\n",
    "\n",
    "Beyond just slowly reducing our learning rate over time - we can use an LR Scheduler to allow us to move our learning according to a desired pattern.\n",
    "\n",
    "We will use a \"cosine with warmup\" schedule and our learning rate, thusly, will follow this pattern:\n",
    "\n",
    "![img](https://i.imgur.com/KoFEl0b.png)\n",
    "\n",
    "There are many different schedulers, and many different ways to handle learning rate, and you can read about just a few of them [here](https://d2l.ai/chapter_optimization/lr-scheduler.html)!"
   ],
   "metadata": {
    "id": "fLsOpaACDDkF"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def get_lr(it):\n",
    "    # 1) linear warmup for warmup_iters steps\n",
    "    if it < warmup_iters:\n",
    "        return learning_rate * it / warmup_iters\n",
    "    # 2) if it > lr_decay_iters, return min learning rate\n",
    "    if it > lr_decay_iters:\n",
    "        return min_lr\n",
    "    # 3) in between, use cosine decay down to min learning rate\n",
    "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1\n",
    "    return min_lr + coeff * (learning_rate - min_lr)"
   ],
   "metadata": {
    "id": "7-mNpWBSWdHh"
   },
   "execution_count": 85,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "We need to set some specific values in our env to allow training in Colab."
   ],
   "metadata": {
    "id": "cqFePCZmE1Lq"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!export LC_ALL=\"en_US.UTF-8\"\n",
    "!export LD_LIBRARY_PATH=\"/usr/lib64-nvidia\"\n",
    "!export LIBRARY_PATH=\"/usr/local/cuda/lib64/stubs\"\n",
    "!ldconfig /usr/lib64-nvidia"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-7nDL6s4YT6E",
    "outputId": "a5e03a2e-a967-403c-897b-3283ebe47401"
   },
   "execution_count": 86,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## The Training Loop\n",
    "\n",
    "Now we can finally grab our first batch and set our initial time to calculate how long our iterations are taking!"
   ],
   "metadata": {
    "id": "Nhqmxeo0Eg0Z"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "X, Y = get_batch('train')\n",
    "t0 = time.time()\n",
    "local_iter_num = 0\n",
    "raw_model = model\n",
    "running_mfu = -1.0 # model flops utilization\n",
    "\n",
    "while True:\n",
    "    # determine and set the learning rate for this iteration\n",
    "    lr = get_lr(iter_num) if decay_lr else learning_rate\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    # evaluate the loss on train/val sets and write checkpoints\n",
    "    if iter_num % eval_interval == 0 and master_process:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "        if losses['val'] < best_val_loss or always_save_checkpoint:\n",
    "            best_val_loss = losses['val']\n",
    "            if iter_num > 0:\n",
    "                checkpoint = {\n",
    "                    'model': raw_model.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'model_args': model_args,\n",
    "                    'iter_num': iter_num,\n",
    "                    'best_val_loss': best_val_loss,\n",
    "                    'config': config,\n",
    "                }\n",
    "                print(f\"saving checkpoint to {out_dir}\")\n",
    "                torch.save(checkpoint, os.path.join(out_dir, 'ckpt.pt'))\n",
    "    if iter_num == 0 and eval_only:\n",
    "        break\n",
    "\n",
    "    # forward backward update, with optional gradient accumulation to simulate larger batch size\n",
    "    # and using the GradScaler if data type is float16\n",
    "    for micro_step in range(gradient_accumulation_steps):\n",
    "        with ctx:\n",
    "            logits, loss = model(X, Y)\n",
    "            loss = loss / gradient_accumulation_steps # scale the loss to account for gradient accumulation\n",
    "        # immediately async prefetch next batch while model is doing the forward pass on the GPU\n",
    "        X, Y = get_batch('train')\n",
    "        # backward pass, with gradient scaling if training in fp16\n",
    "        scaler.scale(loss).backward()\n",
    "    # clip the gradient\n",
    "    if grad_clip != 0.0:\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "    # step the optimizer and scaler if training in fp16\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    # flush the gradients as soon as we can, no need for this memory anymore\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    # timing and logging\n",
    "    t1 = time.time()\n",
    "    dt = t1 - t0\n",
    "    t0 = t1\n",
    "    if iter_num % log_interval == 0 and master_process:\n",
    "        # get loss as float. note: this is a CPU-GPU sync point\n",
    "        # scale up to undo the division above, approximating the true total loss (exact would have been a sum)\n",
    "        lossf = loss.item() * gradient_accumulation_steps\n",
    "        if local_iter_num >= 5: # let the training loop settle a bit\n",
    "            mfu = raw_model.estimate_mfu(batch_size * gradient_accumulation_steps, dt)\n",
    "            running_mfu = mfu if running_mfu == -1.0 else 0.9*running_mfu + 0.1*mfu\n",
    "        print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms, mfu {running_mfu*100:.2f}%\")\n",
    "    iter_num += 1\n",
    "    local_iter_num += 1\n",
    "\n",
    "    # termination conditions\n",
    "    if iter_num > max_iters:\n",
    "        break"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kHbyEapRWmpc",
    "outputId": "8dd7f482-ed99-4ac8-8e73-4b38a4902fe6"
   },
   "execution_count": 87,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "step 0: train loss 10.9197, val loss 10.9168\n",
      "iter 0: loss 10.9182, time 34069.62ms, mfu -100.00%\n",
      "iter 8: loss 9.4089, time 1479.29ms, mfu 3.88%\n",
      "iter 16: loss 8.6422, time 1496.93ms, mfu 3.87%\n",
      "iter 24: loss 7.3445, time 1454.66ms, mfu 3.88%\n",
      "iter 32: loss 6.2699, time 1430.21ms, mfu 3.89%\n",
      "iter 40: loss 5.7469, time 1416.84ms, mfu 3.91%\n",
      "iter 48: loss 5.6467, time 1414.61ms, mfu 3.92%\n",
      "iter 56: loss 5.5441, time 1410.25ms, mfu 3.94%\n",
      "iter 64: loss 5.1902, time 1419.06ms, mfu 3.95%\n",
      "iter 72: loss 4.8112, time 1419.68ms, mfu 3.96%\n",
      "iter 80: loss 4.5580, time 1422.08ms, mfu 3.97%\n",
      "iter 88: loss 4.4954, time 1419.67ms, mfu 3.97%\n",
      "iter 96: loss 4.1598, time 1411.35ms, mfu 3.98%\n",
      "iter 104: loss 4.3928, time 1405.12ms, mfu 3.99%\n",
      "iter 112: loss 4.0279, time 1407.83ms, mfu 4.00%\n",
      "iter 120: loss 4.0642, time 1414.91ms, mfu 4.01%\n",
      "iter 128: loss 3.9797, time 1413.17ms, mfu 4.01%\n",
      "iter 136: loss 3.8089, time 1411.59ms, mfu 4.02%\n",
      "iter 144: loss 3.7710, time 1408.68ms, mfu 4.02%\n",
      "iter 152: loss 3.7476, time 1408.66ms, mfu 4.03%\n",
      "iter 160: loss 3.5548, time 1401.57ms, mfu 4.03%\n",
      "iter 168: loss 3.5476, time 1402.71ms, mfu 4.04%\n",
      "iter 176: loss 3.3491, time 1405.03ms, mfu 4.04%\n",
      "iter 184: loss 3.3161, time 1404.50ms, mfu 4.05%\n",
      "iter 192: loss 3.2931, time 1400.23ms, mfu 4.05%\n",
      "iter 200: loss 3.2963, time 1407.48ms, mfu 4.06%\n",
      "iter 208: loss 3.1296, time 1405.68ms, mfu 4.06%\n",
      "iter 216: loss 3.0986, time 1411.03ms, mfu 4.06%\n",
      "iter 224: loss 3.0042, time 1406.24ms, mfu 4.06%\n",
      "iter 232: loss 2.8505, time 1407.95ms, mfu 4.06%\n",
      "iter 240: loss 2.8980, time 1401.91ms, mfu 4.07%\n",
      "iter 248: loss 2.7138, time 1399.14ms, mfu 4.07%\n",
      "step 250: train loss 2.2979, val loss 5.5497\n",
      "saving checkpoint to out\n",
      "iter 256: loss 2.6163, time 1394.03ms, mfu 4.07%\n",
      "iter 264: loss 2.6066, time 1411.32ms, mfu 4.07%\n",
      "iter 272: loss 2.3756, time 1414.02ms, mfu 4.07%\n",
      "iter 280: loss 2.3409, time 1402.93ms, mfu 4.07%\n",
      "iter 288: loss 2.3107, time 1396.37ms, mfu 4.08%\n",
      "iter 296: loss 2.2236, time 1386.82ms, mfu 4.08%\n",
      "iter 304: loss 2.2509, time 1397.86ms, mfu 4.08%\n",
      "iter 312: loss 2.0371, time 1399.46ms, mfu 4.09%\n",
      "iter 320: loss 2.0415, time 1395.87ms, mfu 4.09%\n",
      "iter 328: loss 1.9948, time 1398.39ms, mfu 4.09%\n",
      "iter 336: loss 1.8745, time 1400.37ms, mfu 4.09%\n",
      "iter 344: loss 1.7921, time 1399.25ms, mfu 4.09%\n",
      "iter 352: loss 1.7713, time 1396.56ms, mfu 4.09%\n",
      "iter 360: loss 1.7407, time 1402.39ms, mfu 4.09%\n",
      "iter 368: loss 1.4896, time 1392.90ms, mfu 4.10%\n",
      "iter 376: loss 1.4901, time 1395.90ms, mfu 4.10%\n",
      "iter 384: loss 1.4931, time 1393.90ms, mfu 4.10%\n",
      "iter 392: loss 1.4228, time 1395.63ms, mfu 4.10%\n",
      "iter 400: loss 1.4114, time 1396.08ms, mfu 4.10%\n",
      "iter 408: loss 1.2937, time 1398.37ms, mfu 4.10%\n",
      "iter 416: loss 1.3051, time 1391.86ms, mfu 4.10%\n",
      "iter 424: loss 1.2283, time 1395.17ms, mfu 4.10%\n",
      "iter 432: loss 1.2096, time 1394.89ms, mfu 4.11%\n",
      "iter 440: loss 1.1607, time 1392.96ms, mfu 4.11%\n",
      "iter 448: loss 1.1449, time 1394.59ms, mfu 4.11%\n",
      "iter 456: loss 1.2291, time 1390.82ms, mfu 4.11%\n",
      "iter 464: loss 1.1545, time 1389.24ms, mfu 4.11%\n",
      "iter 472: loss 1.0858, time 1390.93ms, mfu 4.11%\n",
      "iter 480: loss 1.0562, time 1385.95ms, mfu 4.12%\n",
      "iter 488: loss 0.9859, time 1387.16ms, mfu 4.12%\n",
      "iter 496: loss 1.0338, time 1386.94ms, mfu 4.12%\n",
      "step 500: train loss 0.4520, val loss 7.1738\n",
      "saving checkpoint to out\n",
      "iter 504: loss 1.0009, time 1379.19ms, mfu 4.12%\n",
      "iter 512: loss 0.9640, time 1387.88ms, mfu 4.12%\n",
      "iter 520: loss 0.9435, time 1398.71ms, mfu 4.12%\n",
      "iter 528: loss 0.9321, time 1395.12ms, mfu 4.12%\n",
      "iter 536: loss 0.8836, time 1386.32ms, mfu 4.12%\n",
      "iter 544: loss 0.8565, time 1383.30ms, mfu 4.13%\n",
      "iter 552: loss 0.8535, time 1382.00ms, mfu 4.13%\n",
      "iter 560: loss 0.8492, time 1385.19ms, mfu 4.13%\n",
      "iter 568: loss 0.8029, time 1382.44ms, mfu 4.13%\n",
      "iter 576: loss 0.7507, time 1385.71ms, mfu 4.13%\n",
      "iter 584: loss 0.8027, time 1383.58ms, mfu 4.13%\n",
      "iter 592: loss 0.7904, time 1387.05ms, mfu 4.13%\n",
      "iter 600: loss 0.7918, time 1385.48ms, mfu 4.13%\n",
      "iter 608: loss 0.7387, time 1387.82ms, mfu 4.13%\n",
      "iter 616: loss 0.7257, time 1384.74ms, mfu 4.14%\n",
      "iter 624: loss 0.6987, time 1384.04ms, mfu 4.14%\n",
      "iter 632: loss 0.7044, time 1383.76ms, mfu 4.14%\n",
      "iter 640: loss 0.6920, time 1383.32ms, mfu 4.14%\n",
      "iter 648: loss 0.7186, time 1385.73ms, mfu 4.14%\n",
      "iter 656: loss 0.7211, time 1378.98ms, mfu 4.14%\n",
      "iter 664: loss 0.6876, time 1378.50ms, mfu 4.14%\n",
      "iter 672: loss 0.6692, time 1383.12ms, mfu 4.14%\n",
      "iter 680: loss 0.6318, time 1382.12ms, mfu 4.14%\n",
      "iter 688: loss 0.6215, time 1381.34ms, mfu 4.15%\n",
      "iter 696: loss 0.6370, time 1379.57ms, mfu 4.15%\n",
      "iter 704: loss 0.5869, time 1376.72ms, mfu 4.15%\n",
      "iter 712: loss 0.6139, time 1378.94ms, mfu 4.15%\n",
      "iter 720: loss 0.6551, time 1377.60ms, mfu 4.15%\n",
      "iter 728: loss 0.6216, time 1380.71ms, mfu 4.15%\n",
      "iter 736: loss 0.5855, time 1381.39ms, mfu 4.15%\n",
      "iter 744: loss 0.5949, time 1378.47ms, mfu 4.15%\n",
      "step 750: train loss 0.2202, val loss 8.4536\n",
      "saving checkpoint to out\n",
      "iter 752: loss 0.5698, time 1366.06ms, mfu 4.16%\n",
      "iter 760: loss 0.5758, time 1377.03ms, mfu 4.16%\n",
      "iter 768: loss 0.5629, time 1396.03ms, mfu 4.15%\n",
      "iter 776: loss 0.5417, time 1384.07ms, mfu 4.15%\n",
      "iter 784: loss 0.5384, time 1380.79ms, mfu 4.15%\n",
      "iter 792: loss 0.5780, time 1374.97ms, mfu 4.15%\n",
      "iter 800: loss 0.5544, time 1373.15ms, mfu 4.16%\n",
      "iter 808: loss 0.5679, time 1375.98ms, mfu 4.16%\n",
      "iter 816: loss 0.5115, time 1372.39ms, mfu 4.16%\n",
      "iter 824: loss 0.5323, time 1378.77ms, mfu 4.16%\n",
      "iter 832: loss 0.5011, time 1377.58ms, mfu 4.16%\n",
      "iter 840: loss 0.4982, time 1380.76ms, mfu 4.16%\n",
      "iter 848: loss 0.4771, time 1376.29ms, mfu 4.16%\n",
      "iter 856: loss 0.4947, time 1379.73ms, mfu 4.16%\n",
      "iter 864: loss 0.4734, time 1375.55ms, mfu 4.16%\n",
      "iter 872: loss 0.4903, time 1376.34ms, mfu 4.16%\n",
      "iter 880: loss 0.4907, time 1376.90ms, mfu 4.16%\n",
      "iter 888: loss 0.4815, time 1377.63ms, mfu 4.16%\n",
      "iter 896: loss 0.4701, time 1378.88ms, mfu 4.16%\n",
      "iter 904: loss 0.4667, time 1376.83ms, mfu 4.16%\n",
      "iter 912: loss 0.4672, time 1378.18ms, mfu 4.16%\n",
      "iter 920: loss 0.5034, time 1374.55ms, mfu 4.16%\n",
      "iter 928: loss 0.4846, time 1372.70ms, mfu 4.17%\n",
      "iter 936: loss 0.4193, time 1375.11ms, mfu 4.17%\n",
      "iter 944: loss 0.4688, time 1380.24ms, mfu 4.17%\n",
      "iter 952: loss 0.4464, time 1374.41ms, mfu 4.17%\n",
      "iter 960: loss 0.4176, time 1374.71ms, mfu 4.17%\n",
      "iter 968: loss 0.4111, time 1377.31ms, mfu 4.17%\n",
      "iter 976: loss 0.4124, time 1376.52ms, mfu 4.17%\n",
      "iter 984: loss 0.3710, time 1373.30ms, mfu 4.17%\n",
      "iter 992: loss 0.4177, time 1374.07ms, mfu 4.17%\n",
      "step 1000: train loss 0.1384, val loss 8.8874\n",
      "saving checkpoint to out\n",
      "iter 1000: loss 0.4189, time 8771.70ms, mfu 3.82%\n",
      "iter 1008: loss 0.3998, time 1369.78ms, mfu 3.85%\n",
      "iter 1016: loss 0.3918, time 1382.57ms, mfu 3.88%\n",
      "iter 1024: loss 0.4080, time 1383.53ms, mfu 3.91%\n",
      "iter 1032: loss 0.4041, time 1378.23ms, mfu 3.94%\n",
      "iter 1040: loss 0.4060, time 1374.89ms, mfu 3.96%\n",
      "iter 1048: loss 0.3716, time 1366.82ms, mfu 3.98%\n",
      "iter 1056: loss 0.4016, time 1367.67ms, mfu 4.00%\n",
      "iter 1064: loss 0.3784, time 1371.66ms, mfu 4.02%\n",
      "iter 1072: loss 0.3700, time 1374.68ms, mfu 4.04%\n",
      "iter 1080: loss 0.3749, time 1376.57ms, mfu 4.05%\n",
      "iter 1088: loss 0.3737, time 1373.68ms, mfu 4.06%\n",
      "iter 1096: loss 0.3774, time 1376.16ms, mfu 4.07%\n",
      "iter 1104: loss 0.3813, time 1371.80ms, mfu 4.08%\n",
      "iter 1112: loss 0.3501, time 1374.13ms, mfu 4.09%\n",
      "iter 1120: loss 0.3614, time 1372.95ms, mfu 4.10%\n",
      "iter 1128: loss 0.3625, time 1371.53ms, mfu 4.11%\n",
      "iter 1136: loss 0.3739, time 1374.24ms, mfu 4.12%\n",
      "iter 1144: loss 0.3527, time 1370.40ms, mfu 4.12%\n",
      "iter 1152: loss 0.3572, time 1371.49ms, mfu 4.13%\n",
      "iter 1160: loss 0.3614, time 1370.24ms, mfu 4.14%\n",
      "iter 1168: loss 0.3320, time 1370.74ms, mfu 4.14%\n",
      "iter 1176: loss 0.3391, time 1376.42ms, mfu 4.14%\n",
      "iter 1184: loss 0.3354, time 1370.67ms, mfu 4.15%\n",
      "iter 1192: loss 0.3390, time 1375.34ms, mfu 4.15%\n",
      "iter 1200: loss 0.3580, time 1373.60ms, mfu 4.15%\n",
      "iter 1208: loss 0.3260, time 1372.18ms, mfu 4.16%\n",
      "iter 1216: loss 0.3362, time 1370.24ms, mfu 4.16%\n",
      "iter 1224: loss 0.3168, time 1371.70ms, mfu 4.16%\n",
      "iter 1232: loss 0.3165, time 1369.33ms, mfu 4.16%\n",
      "iter 1240: loss 0.3215, time 1374.23ms, mfu 4.16%\n",
      "iter 1248: loss 0.3183, time 1369.68ms, mfu 4.17%\n",
      "step 1250: train loss 0.1031, val loss 9.1180\n",
      "saving checkpoint to out\n",
      "iter 1256: loss 0.3139, time 1367.72ms, mfu 4.17%\n",
      "iter 1264: loss 0.3419, time 1376.77ms, mfu 4.17%\n",
      "iter 1272: loss 0.3103, time 1386.62ms, mfu 4.17%\n",
      "iter 1280: loss 0.3061, time 1373.54ms, mfu 4.17%\n",
      "iter 1288: loss 0.3186, time 1372.24ms, mfu 4.17%\n",
      "iter 1296: loss 0.3010, time 1368.53ms, mfu 4.17%\n",
      "iter 1304: loss 0.3085, time 1371.68ms, mfu 4.17%\n",
      "iter 1312: loss 0.3103, time 1368.19ms, mfu 4.17%\n",
      "iter 1320: loss 0.3356, time 1375.96ms, mfu 4.17%\n",
      "iter 1328: loss 0.2754, time 1374.19ms, mfu 4.17%\n",
      "iter 1336: loss 0.3129, time 1375.37ms, mfu 4.17%\n",
      "iter 1344: loss 0.2733, time 1367.47ms, mfu 4.18%\n",
      "iter 1352: loss 0.2795, time 1369.70ms, mfu 4.18%\n",
      "iter 1360: loss 0.3318, time 1371.62ms, mfu 4.18%\n",
      "iter 1368: loss 0.2814, time 1372.49ms, mfu 4.18%\n",
      "iter 1376: loss 0.2611, time 1370.74ms, mfu 4.18%\n",
      "iter 1384: loss 0.2902, time 1369.44ms, mfu 4.18%\n",
      "iter 1392: loss 0.2816, time 1374.98ms, mfu 4.18%\n",
      "iter 1400: loss 0.2642, time 1372.33ms, mfu 4.18%\n",
      "iter 1408: loss 0.2712, time 1371.74ms, mfu 4.18%\n",
      "iter 1416: loss 0.2708, time 1370.85ms, mfu 4.18%\n",
      "iter 1424: loss 0.2743, time 1368.75ms, mfu 4.18%\n",
      "iter 1432: loss 0.2567, time 1368.20ms, mfu 4.18%\n",
      "iter 1440: loss 0.2667, time 1369.01ms, mfu 4.18%\n",
      "iter 1448: loss 0.2748, time 1371.35ms, mfu 4.18%\n",
      "iter 1456: loss 0.2443, time 1371.40ms, mfu 4.18%\n",
      "iter 1464: loss 0.2568, time 1370.37ms, mfu 4.18%\n",
      "iter 1472: loss 0.2568, time 1370.03ms, mfu 4.18%\n",
      "iter 1480: loss 0.2661, time 1369.87ms, mfu 4.18%\n",
      "iter 1488: loss 0.2270, time 1370.83ms, mfu 4.18%\n",
      "iter 1496: loss 0.2635, time 1373.37ms, mfu 4.18%\n",
      "step 1500: train loss 0.0766, val loss 9.5086\n",
      "saving checkpoint to out\n",
      "iter 1504: loss 0.2538, time 1365.49ms, mfu 4.19%\n",
      "iter 1512: loss 0.2799, time 1370.99ms, mfu 4.19%\n",
      "iter 1520: loss 0.2524, time 1382.92ms, mfu 4.18%\n",
      "iter 1528: loss 0.2432, time 1372.10ms, mfu 4.18%\n",
      "iter 1536: loss 0.2492, time 1367.63ms, mfu 4.18%\n",
      "iter 1544: loss 0.2553, time 1364.43ms, mfu 4.19%\n",
      "iter 1552: loss 0.2327, time 1366.66ms, mfu 4.19%\n",
      "iter 1560: loss 0.2341, time 1366.04ms, mfu 4.19%\n",
      "iter 1568: loss 0.2471, time 1368.85ms, mfu 4.19%\n",
      "iter 1576: loss 0.2661, time 1373.27ms, mfu 4.19%\n",
      "iter 1584: loss 0.2601, time 1371.59ms, mfu 4.19%\n",
      "iter 1592: loss 0.2533, time 1374.78ms, mfu 4.19%\n",
      "iter 1600: loss 0.2159, time 1371.54ms, mfu 4.19%\n",
      "iter 1608: loss 0.2120, time 1369.44ms, mfu 4.19%\n",
      "iter 1616: loss 0.2076, time 1367.75ms, mfu 4.19%\n",
      "iter 1624: loss 0.2330, time 1366.91ms, mfu 4.19%\n",
      "iter 1632: loss 0.2329, time 1370.80ms, mfu 4.19%\n",
      "iter 1640: loss 0.2544, time 1366.83ms, mfu 4.19%\n",
      "iter 1648: loss 0.2256, time 1367.91ms, mfu 4.19%\n",
      "iter 1656: loss 0.2290, time 1366.75ms, mfu 4.19%\n",
      "iter 1664: loss 0.2232, time 1370.11ms, mfu 4.19%\n",
      "iter 1672: loss 0.2431, time 1369.50ms, mfu 4.19%\n",
      "iter 1680: loss 0.2291, time 1369.70ms, mfu 4.19%\n",
      "iter 1688: loss 0.2249, time 1368.86ms, mfu 4.19%\n",
      "iter 1696: loss 0.2293, time 1370.23ms, mfu 4.19%\n",
      "iter 1704: loss 0.2015, time 1371.71ms, mfu 4.19%\n",
      "iter 1712: loss 0.2183, time 1370.94ms, mfu 4.19%\n",
      "iter 1720: loss 0.2171, time 1367.30ms, mfu 4.19%\n",
      "iter 1728: loss 0.2037, time 1364.61ms, mfu 4.19%\n",
      "iter 1736: loss 0.2199, time 1365.65ms, mfu 4.19%\n",
      "iter 1744: loss 0.2287, time 1366.82ms, mfu 4.19%\n",
      "step 1750: train loss 0.0720, val loss 9.8221\n",
      "saving checkpoint to out\n",
      "iter 1752: loss 0.2286, time 1360.61ms, mfu 4.19%\n",
      "iter 1760: loss 0.2006, time 1369.64ms, mfu 4.19%\n",
      "iter 1768: loss 0.1976, time 1382.65ms, mfu 4.19%\n",
      "iter 1776: loss 0.2103, time 1374.79ms, mfu 4.19%\n",
      "iter 1784: loss 0.2077, time 1370.37ms, mfu 4.19%\n",
      "iter 1792: loss 0.2017, time 1371.42ms, mfu 4.19%\n",
      "iter 1800: loss 0.1970, time 1364.01ms, mfu 4.19%\n",
      "iter 1808: loss 0.1958, time 1362.42ms, mfu 4.19%\n",
      "iter 1816: loss 0.2166, time 1364.72ms, mfu 4.19%\n",
      "iter 1824: loss 0.1925, time 1370.60ms, mfu 4.19%\n",
      "iter 1832: loss 0.1997, time 1371.03ms, mfu 4.19%\n",
      "iter 1840: loss 0.2039, time 1368.85ms, mfu 4.19%\n",
      "iter 1848: loss 0.2014, time 1370.76ms, mfu 4.19%\n",
      "iter 1856: loss 0.1881, time 1368.40ms, mfu 4.19%\n",
      "iter 1864: loss 0.2064, time 1367.00ms, mfu 4.19%\n",
      "iter 1872: loss 0.2104, time 1367.13ms, mfu 4.19%\n",
      "iter 1880: loss 0.2318, time 1372.87ms, mfu 4.19%\n",
      "iter 1888: loss 0.1918, time 1365.24ms, mfu 4.19%\n",
      "iter 1896: loss 0.1909, time 1369.87ms, mfu 4.19%\n",
      "iter 1904: loss 0.2097, time 1371.03ms, mfu 4.19%\n",
      "iter 1912: loss 0.1796, time 1369.15ms, mfu 4.19%\n",
      "iter 1920: loss 0.1912, time 1369.30ms, mfu 4.19%\n",
      "iter 1928: loss 0.1805, time 1368.47ms, mfu 4.19%\n",
      "iter 1936: loss 0.1627, time 1367.38ms, mfu 4.19%\n",
      "iter 1944: loss 0.1824, time 1364.40ms, mfu 4.19%\n",
      "iter 1952: loss 0.2042, time 1362.75ms, mfu 4.19%\n",
      "iter 1960: loss 0.1805, time 1370.97ms, mfu 4.19%\n",
      "iter 1968: loss 0.2039, time 1374.25ms, mfu 4.19%\n",
      "iter 1976: loss 0.1928, time 1367.88ms, mfu 4.19%\n",
      "iter 1984: loss 0.1763, time 1369.11ms, mfu 4.19%\n",
      "iter 1992: loss 0.1787, time 1366.90ms, mfu 4.19%\n",
      "step 2000: train loss 0.0651, val loss 9.8779\n",
      "saving checkpoint to out\n",
      "iter 2000: loss 0.1793, time 8798.44ms, mfu 3.84%\n",
      "iter 2008: loss 0.1686, time 1371.14ms, mfu 3.87%\n",
      "iter 2016: loss 0.1880, time 1379.60ms, mfu 3.90%\n",
      "iter 2024: loss 0.1749, time 1379.46ms, mfu 3.93%\n",
      "iter 2032: loss 0.1609, time 1369.09ms, mfu 3.95%\n",
      "iter 2040: loss 0.1672, time 1365.78ms, mfu 3.98%\n",
      "iter 2048: loss 0.1881, time 1367.79ms, mfu 4.00%\n",
      "iter 2056: loss 0.1791, time 1365.88ms, mfu 4.02%\n",
      "iter 2064: loss 0.1902, time 1368.83ms, mfu 4.04%\n",
      "iter 2072: loss 0.1940, time 1370.20ms, mfu 4.05%\n",
      "iter 2080: loss 0.1572, time 1367.13ms, mfu 4.07%\n",
      "iter 2088: loss 0.1609, time 1370.71ms, mfu 4.08%\n",
      "iter 2096: loss 0.1691, time 1372.30ms, mfu 4.09%\n",
      "iter 2104: loss 0.1601, time 1371.27ms, mfu 4.10%\n",
      "iter 2112: loss 0.1761, time 1374.08ms, mfu 4.11%\n",
      "iter 2120: loss 0.1699, time 1368.82ms, mfu 4.11%\n",
      "iter 2128: loss 0.1534, time 1369.83ms, mfu 4.12%\n",
      "iter 2136: loss 0.1797, time 1363.59ms, mfu 4.13%\n",
      "iter 2144: loss 0.1554, time 1364.19ms, mfu 4.14%\n",
      "iter 2152: loss 0.1629, time 1371.92ms, mfu 4.14%\n",
      "iter 2160: loss 0.1569, time 1375.41ms, mfu 4.15%\n",
      "iter 2168: loss 0.1589, time 1368.67ms, mfu 4.15%\n",
      "iter 2176: loss 0.1548, time 1370.19ms, mfu 4.15%\n",
      "iter 2184: loss 0.1746, time 1369.03ms, mfu 4.16%\n",
      "iter 2192: loss 0.1637, time 1370.37ms, mfu 4.16%\n",
      "iter 2200: loss 0.1648, time 1365.53ms, mfu 4.16%\n",
      "iter 2208: loss 0.1711, time 1370.26ms, mfu 4.17%\n",
      "iter 2216: loss 0.1612, time 1366.23ms, mfu 4.17%\n",
      "iter 2224: loss 0.1607, time 1367.72ms, mfu 4.17%\n",
      "iter 2232: loss 0.1544, time 1368.84ms, mfu 4.17%\n",
      "iter 2240: loss 0.1621, time 1366.81ms, mfu 4.18%\n",
      "iter 2248: loss 0.1534, time 1368.38ms, mfu 4.18%\n",
      "step 2250: train loss 0.0571, val loss 10.0337\n",
      "saving checkpoint to out\n",
      "iter 2256: loss 0.1505, time 1362.20ms, mfu 4.18%\n",
      "iter 2264: loss 0.1519, time 1377.40ms, mfu 4.18%\n",
      "iter 2272: loss 0.1590, time 1381.98ms, mfu 4.18%\n",
      "iter 2280: loss 0.1746, time 1376.64ms, mfu 4.18%\n",
      "iter 2288: loss 0.1384, time 1371.81ms, mfu 4.18%\n",
      "iter 2296: loss 0.1403, time 1365.44ms, mfu 4.18%\n",
      "iter 2304: loss 0.1605, time 1364.75ms, mfu 4.18%\n",
      "iter 2312: loss 0.1606, time 1364.94ms, mfu 4.18%\n",
      "iter 2320: loss 0.1533, time 1373.46ms, mfu 4.18%\n",
      "iter 2328: loss 0.1650, time 1368.59ms, mfu 4.18%\n",
      "iter 2336: loss 0.1491, time 1372.68ms, mfu 4.18%\n",
      "iter 2344: loss 0.1489, time 1368.75ms, mfu 4.18%\n",
      "iter 2352: loss 0.1602, time 1367.76ms, mfu 4.19%\n",
      "iter 2360: loss 0.1432, time 1369.11ms, mfu 4.19%\n",
      "iter 2368: loss 0.1406, time 1366.97ms, mfu 4.19%\n",
      "iter 2376: loss 0.1498, time 1369.54ms, mfu 4.19%\n",
      "iter 2384: loss 0.1425, time 1370.58ms, mfu 4.19%\n",
      "iter 2392: loss 0.1346, time 1367.65ms, mfu 4.19%\n",
      "iter 2400: loss 0.1444, time 1368.95ms, mfu 4.19%\n",
      "iter 2408: loss 0.1490, time 1369.67ms, mfu 4.19%\n",
      "iter 2416: loss 0.1447, time 1372.98ms, mfu 4.19%\n",
      "iter 2424: loss 0.1315, time 1369.61ms, mfu 4.19%\n",
      "iter 2432: loss 0.1395, time 1368.10ms, mfu 4.19%\n",
      "iter 2440: loss 0.1619, time 1364.99ms, mfu 4.19%\n",
      "iter 2448: loss 0.1536, time 1370.00ms, mfu 4.19%\n",
      "iter 2456: loss 0.1345, time 1368.76ms, mfu 4.19%\n",
      "iter 2464: loss 0.1404, time 1371.64ms, mfu 4.19%\n",
      "iter 2472: loss 0.1284, time 1367.57ms, mfu 4.19%\n",
      "iter 2480: loss 0.1343, time 1370.60ms, mfu 4.19%\n",
      "iter 2488: loss 0.1393, time 1368.87ms, mfu 4.19%\n",
      "iter 2496: loss 0.1439, time 1366.48ms, mfu 4.19%\n",
      "step 2500: train loss 0.0580, val loss 10.4051\n",
      "saving checkpoint to out\n",
      "iter 2504: loss 0.1278, time 1362.63ms, mfu 4.19%\n",
      "iter 2512: loss 0.1443, time 1385.90ms, mfu 4.19%\n",
      "iter 2520: loss 0.1336, time 1379.47ms, mfu 4.18%\n",
      "iter 2528: loss 0.1340, time 1368.70ms, mfu 4.18%\n",
      "iter 2536: loss 0.1292, time 1369.33ms, mfu 4.19%\n",
      "iter 2544: loss 0.1416, time 1367.37ms, mfu 4.19%\n",
      "iter 2552: loss 0.1307, time 1368.34ms, mfu 4.19%\n",
      "iter 2560: loss 0.1427, time 1366.17ms, mfu 4.19%\n",
      "iter 2568: loss 0.1361, time 1363.09ms, mfu 4.19%\n",
      "iter 2576: loss 0.1326, time 1367.79ms, mfu 4.19%\n",
      "iter 2584: loss 0.1387, time 1369.29ms, mfu 4.19%\n",
      "iter 2592: loss 0.1274, time 1371.26ms, mfu 4.19%\n",
      "iter 2600: loss 0.1190, time 1368.85ms, mfu 4.19%\n",
      "iter 2608: loss 0.1268, time 1365.93ms, mfu 4.19%\n",
      "iter 2616: loss 0.1241, time 1366.26ms, mfu 4.19%\n",
      "iter 2624: loss 0.1216, time 1369.11ms, mfu 4.19%\n",
      "iter 2632: loss 0.1362, time 1364.48ms, mfu 4.19%\n",
      "iter 2640: loss 0.1428, time 1369.85ms, mfu 4.19%\n",
      "iter 2648: loss 0.1411, time 1367.55ms, mfu 4.19%\n",
      "iter 2656: loss 0.1222, time 1365.11ms, mfu 4.19%\n",
      "iter 2664: loss 0.1277, time 1363.10ms, mfu 4.20%\n",
      "iter 2672: loss 0.1176, time 1371.99ms, mfu 4.19%\n",
      "iter 2680: loss 0.1256, time 1370.32ms, mfu 4.19%\n",
      "iter 2688: loss 0.1318, time 1371.69ms, mfu 4.19%\n",
      "iter 2696: loss 0.1220, time 1370.30ms, mfu 4.19%\n",
      "iter 2704: loss 0.1263, time 1370.59ms, mfu 4.19%\n",
      "iter 2712: loss 0.1274, time 1369.37ms, mfu 4.19%\n",
      "iter 2720: loss 0.1068, time 1367.41ms, mfu 4.19%\n",
      "iter 2728: loss 0.1155, time 1369.55ms, mfu 4.19%\n",
      "iter 2736: loss 0.1201, time 1367.84ms, mfu 4.19%\n",
      "iter 2744: loss 0.1322, time 1367.91ms, mfu 4.19%\n",
      "step 2750: train loss 0.0456, val loss 10.4336\n",
      "saving checkpoint to out\n",
      "iter 2752: loss 0.1081, time 1356.49ms, mfu 4.20%\n",
      "iter 2760: loss 0.1232, time 1364.24ms, mfu 4.20%\n",
      "iter 2768: loss 0.1196, time 1381.84ms, mfu 4.19%\n",
      "iter 2776: loss 0.1195, time 1382.70ms, mfu 4.19%\n",
      "iter 2784: loss 0.1180, time 1376.02ms, mfu 4.19%\n",
      "iter 2792: loss 0.1057, time 1366.13ms, mfu 4.19%\n",
      "iter 2800: loss 0.1382, time 1362.61ms, mfu 4.19%\n",
      "iter 2808: loss 0.1174, time 1360.46ms, mfu 4.19%\n",
      "iter 2816: loss 0.1189, time 1362.10ms, mfu 4.19%\n",
      "iter 2824: loss 0.1122, time 1368.46ms, mfu 4.19%\n",
      "iter 2832: loss 0.1093, time 1374.78ms, mfu 4.19%\n",
      "iter 2840: loss 0.1221, time 1374.09ms, mfu 4.19%\n",
      "iter 2848: loss 0.1176, time 1372.23ms, mfu 4.19%\n",
      "iter 2856: loss 0.1233, time 1370.00ms, mfu 4.19%\n",
      "iter 2864: loss 0.1081, time 1366.83ms, mfu 4.19%\n",
      "iter 2872: loss 0.1089, time 1364.93ms, mfu 4.19%\n",
      "iter 2880: loss 0.1105, time 1371.41ms, mfu 4.19%\n",
      "iter 2888: loss 0.1072, time 1371.22ms, mfu 4.19%\n",
      "iter 2896: loss 0.1118, time 1369.05ms, mfu 4.19%\n",
      "iter 2904: loss 0.0947, time 1365.88ms, mfu 4.19%\n",
      "iter 2912: loss 0.1049, time 1368.05ms, mfu 4.19%\n",
      "iter 2920: loss 0.1067, time 1370.58ms, mfu 4.19%\n",
      "iter 2928: loss 0.1197, time 1368.61ms, mfu 4.19%\n",
      "iter 2936: loss 0.1071, time 1365.94ms, mfu 4.19%\n",
      "iter 2944: loss 0.1100, time 1372.32ms, mfu 4.19%\n",
      "iter 2952: loss 0.1160, time 1369.99ms, mfu 4.19%\n",
      "iter 2960: loss 0.1116, time 1371.29ms, mfu 4.19%\n",
      "iter 2968: loss 0.1013, time 1368.53ms, mfu 4.19%\n",
      "iter 2976: loss 0.1044, time 1371.38ms, mfu 4.19%\n",
      "iter 2984: loss 0.1165, time 1368.47ms, mfu 4.19%\n",
      "iter 2992: loss 0.1201, time 1366.00ms, mfu 4.19%\n",
      "step 3000: train loss 0.0499, val loss 10.6900\n",
      "saving checkpoint to out\n",
      "iter 3000: loss 0.1044, time 8190.03ms, mfu 3.84%\n",
      "iter 3008: loss 0.1075, time 1366.13ms, mfu 3.88%\n",
      "iter 3016: loss 0.1108, time 1376.76ms, mfu 3.91%\n",
      "iter 3024: loss 0.0918, time 1377.67ms, mfu 3.93%\n",
      "iter 3032: loss 0.1032, time 1369.89ms, mfu 3.96%\n",
      "iter 3040: loss 0.1211, time 1367.44ms, mfu 3.98%\n",
      "iter 3048: loss 0.1132, time 1367.77ms, mfu 4.00%\n",
      "iter 3056: loss 0.0948, time 1367.86ms, mfu 4.02%\n",
      "iter 3064: loss 0.1130, time 1367.34ms, mfu 4.04%\n",
      "iter 3072: loss 0.1009, time 1368.94ms, mfu 4.05%\n",
      "iter 3080: loss 0.0889, time 1371.02ms, mfu 4.07%\n",
      "iter 3088: loss 0.1168, time 1371.02ms, mfu 4.08%\n",
      "iter 3096: loss 0.1012, time 1370.52ms, mfu 4.09%\n",
      "iter 3104: loss 0.0885, time 1370.56ms, mfu 4.10%\n",
      "iter 3112: loss 0.0823, time 1364.68ms, mfu 4.11%\n",
      "iter 3120: loss 0.0991, time 1369.07ms, mfu 4.12%\n",
      "iter 3128: loss 0.1019, time 1370.19ms, mfu 4.12%\n",
      "iter 3136: loss 0.1025, time 1365.62ms, mfu 4.13%\n",
      "iter 3144: loss 0.1054, time 1366.56ms, mfu 4.14%\n",
      "iter 3152: loss 0.0928, time 1368.14ms, mfu 4.14%\n",
      "iter 3160: loss 0.1066, time 1365.06ms, mfu 4.15%\n",
      "iter 3168: loss 0.1002, time 1370.99ms, mfu 4.15%\n",
      "iter 3176: loss 0.1036, time 1369.79ms, mfu 4.16%\n",
      "iter 3184: loss 0.0944, time 1368.64ms, mfu 4.16%\n",
      "iter 3192: loss 0.0923, time 1367.72ms, mfu 4.16%\n",
      "iter 3200: loss 0.0848, time 1367.99ms, mfu 4.17%\n",
      "iter 3208: loss 0.1105, time 1368.11ms, mfu 4.17%\n",
      "iter 3216: loss 0.1028, time 1371.52ms, mfu 4.17%\n",
      "iter 3224: loss 0.1022, time 1370.05ms, mfu 4.17%\n",
      "iter 3232: loss 0.0936, time 1368.45ms, mfu 4.17%\n",
      "iter 3240: loss 0.0918, time 1366.82ms, mfu 4.18%\n",
      "iter 3248: loss 0.1089, time 1367.40ms, mfu 4.18%\n",
      "step 3250: train loss 0.0445, val loss 10.8717\n",
      "saving checkpoint to out\n",
      "iter 3256: loss 0.0828, time 1363.19ms, mfu 4.18%\n",
      "iter 3264: loss 0.1011, time 1372.91ms, mfu 4.18%\n",
      "iter 3272: loss 0.0886, time 1379.29ms, mfu 4.18%\n",
      "iter 3280: loss 0.0858, time 1374.13ms, mfu 4.18%\n",
      "iter 3288: loss 0.0927, time 1368.01ms, mfu 4.18%\n",
      "iter 3296: loss 0.0847, time 1366.75ms, mfu 4.18%\n",
      "iter 3304: loss 0.1074, time 1360.43ms, mfu 4.19%\n",
      "iter 3312: loss 0.0967, time 1363.46ms, mfu 4.19%\n",
      "iter 3320: loss 0.0924, time 1368.86ms, mfu 4.19%\n",
      "iter 3328: loss 0.1012, time 1372.42ms, mfu 4.19%\n",
      "iter 3336: loss 0.0902, time 1368.91ms, mfu 4.19%\n",
      "iter 3344: loss 0.0874, time 1368.57ms, mfu 4.19%\n",
      "iter 3352: loss 0.0747, time 1367.72ms, mfu 4.19%\n",
      "iter 3360: loss 0.0834, time 1367.49ms, mfu 4.19%\n",
      "iter 3368: loss 0.0985, time 1365.30ms, mfu 4.19%\n",
      "iter 3376: loss 0.0941, time 1369.75ms, mfu 4.19%\n",
      "iter 3384: loss 0.0784, time 1372.01ms, mfu 4.19%\n",
      "iter 3392: loss 0.0940, time 1371.03ms, mfu 4.19%\n",
      "iter 3400: loss 0.0839, time 1365.97ms, mfu 4.19%\n",
      "iter 3408: loss 0.0854, time 1370.48ms, mfu 4.19%\n",
      "iter 3416: loss 0.0880, time 1369.23ms, mfu 4.19%\n",
      "iter 3424: loss 0.0713, time 1369.73ms, mfu 4.19%\n",
      "iter 3432: loss 0.0808, time 1367.99ms, mfu 4.19%\n",
      "iter 3440: loss 0.0814, time 1369.14ms, mfu 4.19%\n",
      "iter 3448: loss 0.0876, time 1369.43ms, mfu 4.19%\n",
      "iter 3456: loss 0.0925, time 1366.37ms, mfu 4.19%\n",
      "iter 3464: loss 0.0825, time 1364.84ms, mfu 4.19%\n",
      "iter 3472: loss 0.0845, time 1365.95ms, mfu 4.19%\n",
      "iter 3480: loss 0.0916, time 1366.43ms, mfu 4.19%\n",
      "iter 3488: loss 0.0886, time 1366.89ms, mfu 4.19%\n",
      "iter 3496: loss 0.0866, time 1364.10ms, mfu 4.20%\n",
      "step 3500: train loss 0.0401, val loss 10.7794\n",
      "saving checkpoint to out\n",
      "iter 3504: loss 0.0791, time 1356.47ms, mfu 4.20%\n",
      "iter 3512: loss 0.0883, time 1369.79ms, mfu 4.20%\n",
      "iter 3520: loss 0.0793, time 1377.11ms, mfu 4.19%\n",
      "iter 3528: loss 0.0694, time 1369.75ms, mfu 4.19%\n",
      "iter 3536: loss 0.0828, time 1371.02ms, mfu 4.19%\n",
      "iter 3544: loss 0.0871, time 1364.99ms, mfu 4.19%\n",
      "iter 3552: loss 0.0843, time 1363.70ms, mfu 4.20%\n",
      "iter 3560: loss 0.0873, time 1361.53ms, mfu 4.20%\n",
      "iter 3568: loss 0.0807, time 1365.73ms, mfu 4.20%\n",
      "iter 3576: loss 0.0849, time 1367.99ms, mfu 4.20%\n",
      "iter 3584: loss 0.0888, time 1368.90ms, mfu 4.20%\n",
      "iter 3592: loss 0.0950, time 1365.79ms, mfu 4.20%\n",
      "iter 3600: loss 0.0903, time 1369.01ms, mfu 4.20%\n",
      "iter 3608: loss 0.0906, time 1367.13ms, mfu 4.20%\n",
      "iter 3616: loss 0.0811, time 1369.33ms, mfu 4.20%\n",
      "iter 3624: loss 0.0784, time 1367.04ms, mfu 4.20%\n",
      "iter 3632: loss 0.0842, time 1366.50ms, mfu 4.20%\n",
      "iter 3640: loss 0.0836, time 1368.25ms, mfu 4.20%\n",
      "iter 3648: loss 0.0867, time 1369.00ms, mfu 4.20%\n",
      "iter 3656: loss 0.0922, time 1367.56ms, mfu 4.20%\n",
      "iter 3664: loss 0.0759, time 1368.39ms, mfu 4.19%\n",
      "iter 3672: loss 0.0794, time 1366.24ms, mfu 4.20%\n",
      "iter 3680: loss 0.0787, time 1367.95ms, mfu 4.20%\n",
      "iter 3688: loss 0.0725, time 1366.33ms, mfu 4.20%\n",
      "iter 3696: loss 0.0717, time 1361.04ms, mfu 4.20%\n",
      "iter 3704: loss 0.0700, time 1364.04ms, mfu 4.20%\n",
      "iter 3712: loss 0.0765, time 1364.61ms, mfu 4.20%\n",
      "iter 3720: loss 0.0838, time 1360.99ms, mfu 4.20%\n",
      "iter 3728: loss 0.0827, time 1366.03ms, mfu 4.20%\n",
      "iter 3736: loss 0.0856, time 1365.82ms, mfu 4.20%\n",
      "iter 3744: loss 0.0966, time 1367.84ms, mfu 4.20%\n",
      "step 3750: train loss 0.0409, val loss 10.9606\n",
      "saving checkpoint to out\n",
      "iter 3752: loss 0.0790, time 1355.19ms, mfu 4.20%\n",
      "iter 3760: loss 0.0745, time 1369.29ms, mfu 4.20%\n",
      "iter 3768: loss 0.0717, time 1378.52ms, mfu 4.20%\n",
      "iter 3776: loss 0.0855, time 1371.17ms, mfu 4.20%\n",
      "iter 3784: loss 0.0850, time 1363.44ms, mfu 4.20%\n",
      "iter 3792: loss 0.0806, time 1364.43ms, mfu 4.20%\n",
      "iter 3800: loss 0.0722, time 1362.29ms, mfu 4.20%\n",
      "iter 3808: loss 0.0783, time 1365.34ms, mfu 4.20%\n",
      "iter 3816: loss 0.0836, time 1365.82ms, mfu 4.20%\n",
      "iter 3824: loss 0.0736, time 1367.60ms, mfu 4.20%\n",
      "iter 3832: loss 0.0694, time 1368.95ms, mfu 4.20%\n",
      "iter 3840: loss 0.0769, time 1371.65ms, mfu 4.20%\n",
      "iter 3848: loss 0.0743, time 1368.61ms, mfu 4.20%\n",
      "iter 3856: loss 0.0645, time 1369.48ms, mfu 4.20%\n",
      "iter 3864: loss 0.0669, time 1367.59ms, mfu 4.20%\n",
      "iter 3872: loss 0.0762, time 1365.98ms, mfu 4.20%\n",
      "iter 3880: loss 0.0721, time 1367.10ms, mfu 4.20%\n",
      "iter 3888: loss 0.0825, time 1363.47ms, mfu 4.20%\n",
      "iter 3896: loss 0.0764, time 1364.92ms, mfu 4.20%\n",
      "iter 3904: loss 0.0677, time 1364.40ms, mfu 4.20%\n",
      "iter 3912: loss 0.0774, time 1364.04ms, mfu 4.20%\n",
      "iter 3920: loss 0.0829, time 1367.96ms, mfu 4.20%\n",
      "iter 3928: loss 0.0787, time 1361.29ms, mfu 4.20%\n",
      "iter 3936: loss 0.0759, time 1364.05ms, mfu 4.20%\n",
      "iter 3944: loss 0.0829, time 1365.59ms, mfu 4.20%\n",
      "iter 3952: loss 0.0754, time 1363.30ms, mfu 4.20%\n",
      "iter 3960: loss 0.0784, time 1360.20ms, mfu 4.20%\n",
      "iter 3968: loss 0.0698, time 1365.40ms, mfu 4.20%\n",
      "iter 3976: loss 0.0818, time 1366.39ms, mfu 4.20%\n",
      "iter 3984: loss 0.0754, time 1364.21ms, mfu 4.20%\n",
      "iter 3992: loss 0.0687, time 1364.11ms, mfu 4.20%\n",
      "step 4000: train loss 0.0393, val loss 10.7752\n",
      "saving checkpoint to out\n",
      "iter 4000: loss 0.0717, time 8782.02ms, mfu 3.85%\n",
      "iter 4008: loss 0.0644, time 1365.34ms, mfu 3.88%\n",
      "iter 4016: loss 0.0804, time 1377.94ms, mfu 3.91%\n",
      "iter 4024: loss 0.0719, time 1377.77ms, mfu 3.94%\n",
      "iter 4032: loss 0.0743, time 1370.06ms, mfu 3.96%\n",
      "iter 4040: loss 0.0726, time 1367.20ms, mfu 3.99%\n",
      "iter 4048: loss 0.0789, time 1364.14ms, mfu 4.01%\n",
      "iter 4056: loss 0.0763, time 1366.48ms, mfu 4.03%\n",
      "iter 4064: loss 0.0707, time 1368.36ms, mfu 4.04%\n",
      "iter 4072: loss 0.0624, time 1370.46ms, mfu 4.06%\n",
      "iter 4080: loss 0.0659, time 1373.81ms, mfu 4.07%\n",
      "iter 4088: loss 0.0704, time 1372.03ms, mfu 4.08%\n",
      "iter 4096: loss 0.0757, time 1368.26ms, mfu 4.09%\n",
      "iter 4104: loss 0.0795, time 1369.85ms, mfu 4.10%\n",
      "iter 4112: loss 0.0716, time 1369.67ms, mfu 4.11%\n",
      "iter 4120: loss 0.0622, time 1366.52ms, mfu 4.12%\n",
      "iter 4128: loss 0.0578, time 1369.53ms, mfu 4.13%\n",
      "iter 4136: loss 0.0735, time 1368.37ms, mfu 4.13%\n",
      "iter 4144: loss 0.0784, time 1369.51ms, mfu 4.14%\n",
      "iter 4152: loss 0.0635, time 1365.93ms, mfu 4.14%\n",
      "iter 4160: loss 0.0697, time 1368.14ms, mfu 4.15%\n",
      "iter 4168: loss 0.0607, time 1367.15ms, mfu 4.15%\n",
      "iter 4176: loss 0.0736, time 1365.51ms, mfu 4.16%\n",
      "iter 4184: loss 0.0718, time 1364.68ms, mfu 4.16%\n",
      "iter 4192: loss 0.0645, time 1365.44ms, mfu 4.17%\n",
      "iter 4200: loss 0.0757, time 1362.83ms, mfu 4.17%\n",
      "iter 4208: loss 0.0690, time 1366.13ms, mfu 4.17%\n",
      "iter 4216: loss 0.0638, time 1365.67ms, mfu 4.18%\n",
      "iter 4224: loss 0.0758, time 1366.46ms, mfu 4.18%\n",
      "iter 4232: loss 0.0722, time 1369.18ms, mfu 4.18%\n",
      "iter 4240: loss 0.0617, time 1369.35ms, mfu 4.18%\n",
      "iter 4248: loss 0.0849, time 1370.24ms, mfu 4.18%\n",
      "step 4250: train loss 0.0385, val loss 11.0286\n",
      "saving checkpoint to out\n",
      "iter 4256: loss 0.0678, time 1365.20ms, mfu 4.18%\n",
      "iter 4264: loss 0.0701, time 1373.44ms, mfu 4.18%\n",
      "iter 4272: loss 0.0693, time 1375.62ms, mfu 4.18%\n",
      "iter 4280: loss 0.0696, time 1365.81ms, mfu 4.18%\n",
      "iter 4288: loss 0.0672, time 1367.99ms, mfu 4.18%\n",
      "iter 4296: loss 0.0672, time 1363.81ms, mfu 4.19%\n",
      "iter 4304: loss 0.0807, time 1364.25ms, mfu 4.19%\n",
      "iter 4312: loss 0.0680, time 1365.25ms, mfu 4.19%\n",
      "iter 4320: loss 0.0684, time 1367.16ms, mfu 4.19%\n",
      "iter 4328: loss 0.0775, time 1366.34ms, mfu 4.19%\n",
      "iter 4336: loss 0.0856, time 1367.79ms, mfu 4.19%\n",
      "iter 4344: loss 0.0664, time 1362.36ms, mfu 4.19%\n",
      "iter 4352: loss 0.0682, time 1368.30ms, mfu 4.19%\n",
      "iter 4360: loss 0.0622, time 1365.12ms, mfu 4.19%\n",
      "iter 4368: loss 0.0672, time 1367.47ms, mfu 4.19%\n",
      "iter 4376: loss 0.0613, time 1366.21ms, mfu 4.20%\n",
      "iter 4384: loss 0.0607, time 1366.64ms, mfu 4.20%\n",
      "iter 4392: loss 0.0571, time 1370.13ms, mfu 4.19%\n",
      "iter 4400: loss 0.0612, time 1366.13ms, mfu 4.20%\n",
      "iter 4408: loss 0.0627, time 1365.41ms, mfu 4.20%\n",
      "iter 4416: loss 0.0582, time 1368.94ms, mfu 4.20%\n",
      "iter 4424: loss 0.0662, time 1369.01ms, mfu 4.19%\n",
      "iter 4432: loss 0.0624, time 1362.09ms, mfu 4.20%\n",
      "iter 4440: loss 0.0620, time 1366.41ms, mfu 4.20%\n",
      "iter 4448: loss 0.0680, time 1369.37ms, mfu 4.20%\n",
      "iter 4456: loss 0.0757, time 1367.85ms, mfu 4.20%\n",
      "iter 4464: loss 0.0694, time 1365.62ms, mfu 4.20%\n",
      "iter 4472: loss 0.0614, time 1367.89ms, mfu 4.20%\n",
      "iter 4480: loss 0.0690, time 1365.85ms, mfu 4.20%\n",
      "iter 4488: loss 0.0671, time 1369.10ms, mfu 4.20%\n",
      "iter 4496: loss 0.0664, time 1366.83ms, mfu 4.20%\n",
      "step 4500: train loss 0.0351, val loss 11.0588\n",
      "saving checkpoint to out\n",
      "iter 4504: loss 0.0689, time 1360.17ms, mfu 4.20%\n",
      "iter 4512: loss 0.0662, time 1370.55ms, mfu 4.20%\n",
      "iter 4520: loss 0.0765, time 1378.50ms, mfu 4.19%\n",
      "iter 4528: loss 0.0600, time 1372.54ms, mfu 4.19%\n",
      "iter 4536: loss 0.0527, time 1369.89ms, mfu 4.19%\n",
      "iter 4544: loss 0.0611, time 1367.65ms, mfu 4.19%\n",
      "iter 4552: loss 0.0607, time 1362.32ms, mfu 4.19%\n",
      "iter 4560: loss 0.0675, time 1361.92ms, mfu 4.20%\n",
      "iter 4568: loss 0.0626, time 1367.53ms, mfu 4.20%\n",
      "iter 4576: loss 0.0647, time 1370.92ms, mfu 4.19%\n",
      "iter 4584: loss 0.0699, time 1371.60ms, mfu 4.19%\n",
      "iter 4592: loss 0.0712, time 1367.51ms, mfu 4.19%\n",
      "iter 4600: loss 0.0647, time 1366.34ms, mfu 4.19%\n",
      "iter 4608: loss 0.0581, time 1363.64ms, mfu 4.20%\n",
      "iter 4616: loss 0.0657, time 1362.40ms, mfu 4.20%\n",
      "iter 4624: loss 0.0648, time 1366.08ms, mfu 4.20%\n",
      "iter 4632: loss 0.0603, time 1363.90ms, mfu 4.20%\n",
      "iter 4640: loss 0.0693, time 1367.84ms, mfu 4.20%\n",
      "iter 4648: loss 0.0703, time 1367.50ms, mfu 4.20%\n",
      "iter 4656: loss 0.0689, time 1369.09ms, mfu 4.20%\n",
      "iter 4664: loss 0.0548, time 1364.14ms, mfu 4.20%\n",
      "iter 4672: loss 0.0603, time 1364.59ms, mfu 4.20%\n",
      "iter 4680: loss 0.0672, time 1369.79ms, mfu 4.20%\n",
      "iter 4688: loss 0.0696, time 1367.43ms, mfu 4.20%\n",
      "iter 4696: loss 0.0673, time 1367.93ms, mfu 4.20%\n",
      "iter 4704: loss 0.0636, time 1367.67ms, mfu 4.20%\n",
      "iter 4712: loss 0.0695, time 1365.43ms, mfu 4.20%\n",
      "iter 4720: loss 0.0620, time 1365.30ms, mfu 4.20%\n",
      "iter 4728: loss 0.0544, time 1365.38ms, mfu 4.20%\n",
      "iter 4736: loss 0.0618, time 1369.59ms, mfu 4.20%\n",
      "iter 4744: loss 0.0641, time 1371.28ms, mfu 4.20%\n",
      "step 4750: train loss 0.0355, val loss 10.7853\n",
      "saving checkpoint to out\n",
      "iter 4752: loss 0.0693, time 1359.23ms, mfu 4.20%\n",
      "iter 4760: loss 0.0646, time 1365.04ms, mfu 4.20%\n",
      "iter 4768: loss 0.0594, time 1381.80ms, mfu 4.19%\n",
      "iter 4776: loss 0.0658, time 1375.91ms, mfu 4.19%\n",
      "iter 4784: loss 0.0641, time 1370.29ms, mfu 4.19%\n",
      "iter 4792: loss 0.0593, time 1363.73ms, mfu 4.19%\n",
      "iter 4800: loss 0.0677, time 1359.74ms, mfu 4.20%\n",
      "iter 4808: loss 0.0579, time 1361.71ms, mfu 4.20%\n",
      "iter 4816: loss 0.0687, time 1367.28ms, mfu 4.20%\n",
      "iter 4824: loss 0.0618, time 1369.57ms, mfu 4.20%\n",
      "iter 4832: loss 0.0681, time 1367.21ms, mfu 4.20%\n",
      "iter 4840: loss 0.0729, time 1367.17ms, mfu 4.20%\n",
      "iter 4848: loss 0.0660, time 1366.28ms, mfu 4.20%\n",
      "iter 4856: loss 0.0654, time 1364.04ms, mfu 4.20%\n",
      "iter 4864: loss 0.0591, time 1364.54ms, mfu 4.20%\n",
      "iter 4872: loss 0.0668, time 1365.06ms, mfu 4.20%\n",
      "iter 4880: loss 0.0533, time 1372.05ms, mfu 4.20%\n",
      "iter 4888: loss 0.0607, time 1366.79ms, mfu 4.20%\n",
      "iter 4896: loss 0.0659, time 1366.18ms, mfu 4.20%\n",
      "iter 4904: loss 0.0547, time 1365.11ms, mfu 4.20%\n",
      "iter 4912: loss 0.0649, time 1363.31ms, mfu 4.20%\n",
      "iter 4920: loss 0.0609, time 1369.55ms, mfu 4.20%\n",
      "iter 4928: loss 0.0551, time 1365.72ms, mfu 4.20%\n",
      "iter 4936: loss 0.0682, time 1368.82ms, mfu 4.20%\n",
      "iter 4944: loss 0.0647, time 1369.86ms, mfu 4.20%\n",
      "iter 4952: loss 0.0686, time 1369.22ms, mfu 4.20%\n",
      "iter 4960: loss 0.0604, time 1370.69ms, mfu 4.19%\n",
      "iter 4968: loss 0.0729, time 1370.28ms, mfu 4.19%\n",
      "iter 4976: loss 0.0613, time 1371.15ms, mfu 4.19%\n",
      "iter 4984: loss 0.0643, time 1368.51ms, mfu 4.19%\n",
      "iter 4992: loss 0.0723, time 1370.56ms, mfu 4.19%\n",
      "step 5000: train loss 0.0387, val loss 11.0179\n",
      "saving checkpoint to out\n",
      "iter 5000: loss 0.0639, time 8850.84ms, mfu 3.84%\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Generating Outputs with our New Model\n",
    "\n",
    "Now we can leverage the `sample.py` file to generate outputs from our model!"
   ],
   "metadata": {
    "id": "L2J5JlRxFJOM"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Generation Set Up and Model Loading"
   ],
   "metadata": {
    "id": "eo_QP1ITFfX2"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pickle\n",
    "from contextlib import nullcontext\n",
    "import torch\n",
    "import tiktoken\n",
    "from model import GPTConfig, GPT\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "init_from = 'resume' # either 'resume' (from an out_dir) or a gpt2 variant (e.g. 'gpt2-xl')\n",
    "out_dir = 'out' # ignored if init_from is not 'resume'\n",
    "start = \"\\n\" # or \"<|endoftext|>\" or etc. Can also specify a file, use as: \"FILE:prompt.txt\"\n",
    "num_samples = 10 # number of samples to draw\n",
    "max_new_tokens = 500 # number of tokens generated in each sample\n",
    "temperature = 0.8 # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n",
    "top_k = 200 # retain only the top_k most likely tokens, clamp others to have 0 probability\n",
    "seed = 1337\n",
    "device = 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1', etc.\n",
    "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32' or 'bfloat16' or 'float16'\n",
    "compile = False # use PyTorch 2.0 to compile the model to be faster\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
    "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)"
   ],
   "metadata": {
    "id": "-vftqU9LheEK"
   },
   "execution_count": 88,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# model\n",
    "if init_from == 'resume':\n",
    "    # init from a model saved in a specific directory\n",
    "    ckpt_path = os.path.join(out_dir, 'ckpt.pt')\n",
    "    checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "    gptconf = GPTConfig(**checkpoint['model_args'])\n",
    "    model = GPT(gptconf)\n",
    "    state_dict = checkpoint['model']\n",
    "    unwanted_prefix = '_orig_mod.'\n",
    "    for k,v in list(state_dict.items()):\n",
    "        if k.startswith(unwanted_prefix):\n",
    "            state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "    model.load_state_dict(state_dict)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FQRB3j7iiNkl",
    "outputId": "448c5cce-1449-4d3b-a5ec-c731f4030841"
   },
   "execution_count": 89,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "number of parameters: 43.23M\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "model.eval()\n",
    "model.to(device)\n",
    "if compile:\n",
    "    model = torch.compile(model) # requires PyTorch 2.0 (optional)"
   ],
   "metadata": {
    "id": "N1YAy8DriVZZ"
   },
   "execution_count": 90,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "enc = tokenizer\n",
    "encode = lambda s: enc.encode(s)\n",
    "decode = lambda l: enc.decode(l)"
   ],
   "metadata": {
    "id": "KoB-5ZuLicAT"
   },
   "execution_count": 91,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Generation!"
   ],
   "metadata": {
    "id": "mkTQ9wo7FjYU"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# encode the beginning of the prompt\n",
    "if start.startswith('FILE:'):\n",
    "    with open(start[5:], 'r', encoding='utf-8') as f:\n",
    "        start = f.read()\n",
    "start_ids = encode(start)\n",
    "x = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n",
    "\n",
    "# run generation\n",
    "with torch.no_grad():\n",
    "    with ctx:\n",
    "        for k in range(num_samples):\n",
    "            y = model.generate(x, max_new_tokens, temperature=temperature, top_k=top_k)\n",
    "            print(decode(y[0].tolist()))\n",
    "            print('---------------')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NmTcaHCjii5l",
    "outputId": "5ab4c58f-f184-4b10-a636-72bf81e45b15"
   },
   "execution_count": 94,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n",
      "BRUTUS:\n",
      "Dismiss them home.\n",
      "Here comes his mother.\n",
      "\n",
      "SICINIUS:\n",
      "Let's not meet her.\n",
      "\n",
      "BRUTUS:\n",
      "Why?\n",
      "\n",
      "SICINIUS:\n",
      "They say she's mad.\n",
      "\n",
      "BRUTUS:\n",
      "They have ta'en note of us: keep on your way.\n",
      "\n",
      "VOLUMNIA:\n",
      "O, ye're well met: the hoarded plague o' the gods\n",
      "Requite your love!\n",
      "\n",
      "MENENIUS:\n",
      "Peace, peace; be not so loud.\n",
      "\n",
      "VOLUMNIA:\n",
      "If that I could for weeping, you should hear,--\n",
      "Nay, and you shall hear some.\n",
      "Will you be gone?\n",
      "\n",
      "VIRGILIA:\n",
      "\n",
      "SICINIUS:\n",
      "Are you mankind?\n",
      "\n",
      "VOLUMNIA:\n",
      "Ay, fool; is that a shame? Note but this fool.\n",
      "Was not a man my father? Hadst thou foxship\n",
      "To banish him that struck more blows for Rome\n",
      "Than thou hast spoken words?\n",
      "\n",
      "SICINIUS:\n",
      "O blessed heavens!\n",
      "\n",
      "VOLUMNIA:\n",
      "More noble blows than ever thou wise words;\n",
      "And for Rome's good. I'll tell thee what; yet go:\n",
      "Nay, but thou shalt stay too: I would my son\n",
      "Were in Arabia, and thy tribe before him,\n",
      "His good sword in his hand.\n",
      "\n",
      "SICINIUS:\n",
      "What then?\n",
      "\n",
      "VIRGILIA:\n",
      "What then!\n",
      "He'ld make an end of thy posterity.\n",
      "\n",
      "VOLUMNIA:\n",
      "Bastards and all.\n",
      "Good man, the wounds that he does bear for Rome!\n",
      "\n",
      "MENENIUS:\n",
      "Come, come, peace.\n",
      "SICINIUS:\n",
      "I would he had continued to his country\n",
      "As he began, and not unknit himself\n",
      "The noble knot he made.\n",
      "\n",
      "BRUTUS:\n",
      "I would he had.\n",
      "\n",
      "VOLUMNIA:\n",
      "'I would he had'! 'Twas you incensed the rabble:\n",
      "Cats, that can judge as fitly of his worth\n",
      "As I can of those mysteries which heaven\n",
      "Will not have earth to know.\n",
      "\n",
      "BRUTUS:\n",
      "Pray, let us go.\n",
      "\n",
      "VOLUMNIA:\n",
      "Now, pray, sir, get you gone:\n",
      "You have done a brave deed. Ere you go, hear this:--\n",
      "\n",
      "---------------\n",
      "\n",
      "\n",
      "Second Citizen:\n",
      "You shall ha' it, worthy sir.\n",
      "CORIOLANUS:\n",
      "A match, sir. There's in all two worthy voices\n",
      "begged. I have your alms: adieu.\n",
      "\n",
      "Third Citizen:\n",
      "But this is something odd.\n",
      "\n",
      "Second Citizen:\n",
      "An 'twere to give again,--but 'tis no matter.\n",
      "\n",
      "CORIOLANUS:\n",
      "Pray you now, if it may stand with the tune of your\n",
      "voices that I may be consul, I have here the\n",
      "customary gown.\n",
      "\n",
      "Fourth Citizen:\n",
      "You have deserved nobly of your country, and you\n",
      "have not deserved nobly.\n",
      "\n",
      "CORIOLANUS:\n",
      "Your enigma?\n",
      "\n",
      "Fourth Citizen:\n",
      "You have been a scourge to her enemies, you have\n",
      "been a rod to her friends; you have not indeed loved\n",
      "the common people.\n",
      "\n",
      "CORIOLANUS:\n",
      "You should account me the more virtuous that I have\n",
      "not been common in my love. I will, sir, flatter my\n",
      "sworn brother, the people, to earn a dearer\n",
      "estimation of them; 'tis a condition they account\n",
      "gentle: and since the wisdom of their choice is\n",
      "rather to have my hat than my heart, I will practise\n",
      "the insinuating nod and be off to them most\n",
      "counterfeitly; that is, sir, I will counterfeit the\n",
      "bewitchment of some popular man and give it\n",
      "bountiful to the desirers. Therefore, beseech you,\n",
      "I may be consul.\n",
      "Fifth Citizen:\n",
      "We hope to find you our friend; and therefore give\n",
      "you our voices heartily.\n",
      "\n",
      "Fourth Citizen:\n",
      "You have received many wounds for your country.\n",
      "\n",
      "CORIOLANUS:\n",
      "I will not seal your knowledge with showing them. I\n",
      "will make much of your voices, and so trouble you no further.\n",
      "\n",
      "Both Citizens:\n",
      "The gods give you joy, sir, heartily!\n",
      "\n",
      "CORIOLANUS:\n",
      "Most sweet voices!\n",
      "Better it is to die, better to starve,\n",
      "Than crave the hire which first we do deserve.\n",
      "Why in this woolvish toge should I stand here,\n",
      "To beg of Hob and Dick, that do appear,\n",
      "Their needless vouches? Custom calls me to\n",
      "---------------\n",
      "\n",
      "\n",
      "LADY ANNE:\n",
      "Their aunt I am in law, in love their mother:\n",
      "Then bring me to their sights; I'll bear thy blame\n",
      "And take thy office from thee, on my peril.\n",
      "\n",
      "BRAKENBURY:\n",
      "No, madam, no; I may not leave it so:\n",
      "I am bound by oath, and therefore pardon me.\n",
      "\n",
      "LORD STANLEY:\n",
      "Let me but meet you, ladies, one hour hence,\n",
      "And I'll salute your grace of York as mother,\n",
      "And reverend looker on, of two fair queens.\n",
      "Come, madam, you must straight to Westminster,\n",
      "There to be crowned Richard's royal queen.\n",
      "\n",
      "QUEEN ELIZABETH:\n",
      "O, cut my lace in sunder, that my pent heart\n",
      "May have some scope to beat, or else I swoon\n",
      "With this dead-killing news!\n",
      "\n",
      "LADY ANNE:\n",
      "Despiteful tidings! O unpleasing news!\n",
      "\n",
      "DORSET:\n",
      "Be of good cheer: mother, how fares your grace?\n",
      "\n",
      "QUEEN ELIZABETH:\n",
      "O Dorset, speak not to me, get thee hence!\n",
      "Death and destruction dog thee at the heels;\n",
      "Thy mother's name is ominous to children.\n",
      "If thou wilt outstrip death, go cross the seas,\n",
      "And live with Richmond, from the reach of hell\n",
      "Go, hie thee, hie thee from this slaughter-house,\n",
      "Lest thou increase the number of the dead;\n",
      "And make me die the thrall of Margaret's curse,\n",
      "Nor mother, wife, nor England's counted queen.\n",
      "LORD STANLEY:\n",
      "Full of wise care is this your counsel, madam.\n",
      "Take all the swift advantage of the hours;\n",
      "You shall have letters from me to my son\n",
      "To meet you on the way, and welcome you.\n",
      "Be not ta'en tardy by unwise delay.\n",
      "\n",
      "DUCHESS OF YORK:\n",
      "O ill-dispersing wind of misery!\n",
      "O my accursed womb, the bed of death!\n",
      "A cockatrice hast thou hatch'd to the world,\n",
      "Whose unavoided eye is murderous.\n",
      "\n",
      "LORD STANLEY:\n",
      "Come, madam, come; I in all haste was sent.\n",
      "\n",
      "LADY ANNE:\n",
      "And I in all unwillingness will go.\n",
      "I would to God that the inclusive verge\n",
      "Of\n",
      "---------------\n",
      "\n",
      "ISABELLA:\n",
      "But might you do't, and do the world no wrong,\n",
      "If so your heart were touch'd with that remorse\n",
      "As mine is to him?\n",
      "\n",
      "ANGELO:\n",
      "He's sentenced; 'tis too late.\n",
      "\n",
      "LUCIO:\n",
      "ISABELLA:\n",
      "Too late? why, no; I, that do speak a word.\n",
      "May call it back again. Well, believe this,\n",
      "No ceremony that to great ones 'longs,\n",
      "Not the king's crown, nor the deputed sword,\n",
      "The marshal's truncheon, nor the judge's robe,\n",
      "Become them with one half so good a grace\n",
      "As mercy does.\n",
      "If he had been as you and you as he,\n",
      "You would have slipt like him; but he, like you,\n",
      "Would not have been so stern.\n",
      "\n",
      "ANGELO:\n",
      "Pray you, be gone.\n",
      "\n",
      "ISABELLA:\n",
      "I would to heaven I had your potency,\n",
      "And you were Isabel! should it then be thus?\n",
      "No; I would tell what 'twere to be a judge,\n",
      "And what a prisoner.\n",
      "\n",
      "LUCIO:\n",
      "ANGELO:\n",
      "Your brother is a forfeit of the law,\n",
      "And you but waste your words.\n",
      "\n",
      "ISABELLA:\n",
      "Alas, alas!\n",
      "Why, all the souls that were forfeit once;\n",
      "And He that might the vantage best have took\n",
      "Found out the remedy. How would you be,\n",
      "If He, which is the top of judgment, should\n",
      "But judge you as you are? O, think on that;\n",
      "And mercy then will breathe within your lips,\n",
      "Like man new made.\n",
      "\n",
      "ANGELO:\n",
      "Be you content, fair maid;\n",
      "It is the law, not I condemn your brother:\n",
      "Were he my kinsman, brother, or my son,\n",
      "It should be thus with him: he must die tomorrow.\n",
      "\n",
      "ISABELLA:\n",
      "To-morrow! O, that's sudden! Spare him, spare him!\n",
      "He's not prepared for death. Even for our kitchens\n",
      "We kill the fowl of season: shall we serve heaven\n",
      "With less respect than we do minister\n",
      "To our gross selves? Good, good my lord, bethink you;\n",
      "Who is it that hath\n",
      "---------------\n",
      "\n",
      "CORIOLANUS:\n",
      "Let me o' the people!\n",
      "\n",
      "MENENIUS:\n",
      "Well, sir.\n",
      "\n",
      "CORIOLANUS:\n",
      "What is this?\n",
      "\n",
      "MENENIUS:\n",
      "Is this the promise that you made your mother?\n",
      "\n",
      "COMINIUS:\n",
      "Know, I pray you,--\n",
      "\n",
      "CORIOLANUS:\n",
      "I know no further:\n",
      "Let them pronounce the steep Tarpeian death,\n",
      "Vagabond exile, raying, pent to linger\n",
      "But with a grain a day, I would not buy\n",
      "Their mercy at the price of one fair word;\n",
      "Nor cheque my courage for what they can give,\n",
      "To have't with saying 'Good morrow.'\n",
      "\n",
      "SICINIUS:\n",
      "For that he has,\n",
      "As much as in him lies, from time to time\n",
      "Envied against the people, seeking means\n",
      "To pluck away their power, as now at last\n",
      "Given hostile strokes, and that not in the presence\n",
      "Of dreaded justice, but on the ministers\n",
      "That do distribute it; in the name o' the people\n",
      "And in the power of us the tribunes, we,\n",
      "Even from this instant, banish him our city,\n",
      "In peril of precipitation\n",
      "From off the rock Tarpeian never more\n",
      "To enter our Rome gates: i' the people's name,\n",
      "I say it shall be so.\n",
      "Citizens:\n",
      "It shall be so, it shall be so; let him away:\n",
      "He's banish'd, and it shall be so.\n",
      "\n",
      "COMINIUS:\n",
      "Hear me, my masters, and my common friends,--\n",
      "\n",
      "SICINIUS:\n",
      "He's sentenced; no more hearing.\n",
      "\n",
      "COMINIUS:\n",
      "Let me speak:\n",
      "I have been consul, and can show for Rome\n",
      "Her enemies' marks upon me. I do love\n",
      "My country's good with a respect more tender,\n",
      "More holy and profound, than mine own life,\n",
      "My dear wife's estimate, her womb's increase,\n",
      "And treasure of my loins; then if I would\n",
      "Speak that,--\n",
      "\n",
      "SICINIUS:\n",
      "We know your drift: speak what?\n",
      "\n",
      "BRUTUS:\n",
      "There's no more to be said, but he is banish'd,\n",
      "As enemy to the people and his country:\n",
      "It shall be so.\n",
      "Citizens:\n",
      "It shall be so\n",
      "---------------\n",
      "\n",
      "\n",
      "Second Servingman:\n",
      "Worth six on him.\n",
      "\n",
      "First Servingman:\n",
      "Nay, not so neither: but I take him to be the\n",
      "greater soldier.\n",
      "\n",
      "Second Servingman:\n",
      "Faith, look you, one cannot tell how to say that:\n",
      "for the defence of a town, our general is excellent.\n",
      "\n",
      "First Servingman:\n",
      "Ay, and for an assault too.\n",
      "\n",
      "Third Servingman:\n",
      "O slaves, I can tell you news,-- news, you rascals!\n",
      "\n",
      "First Servingman:\n",
      "What, what, what? let's partake.\n",
      "\n",
      "Third Servingman:\n",
      "I would not be a Roman, of all nations; I had as\n",
      "lieve be a condemned man.\n",
      "\n",
      "First Servingman:\n",
      "Wherefore? wherefore?\n",
      "\n",
      "Third Servingman:\n",
      "Why, here's he that was wont to thwack our general,\n",
      "Caius Marcius.\n",
      "\n",
      "First Servingman:\n",
      "Why do you say 'thwack our general '?\n",
      "\n",
      "Third Servingman:\n",
      "I do not say 'thwack our general;' but he was always\n",
      "good enough for him.\n",
      "\n",
      "Second Servingman:\n",
      "Come, we are fellows and friends: he was ever too\n",
      "hard for him; I have heard him say so himself.\n",
      "First Servingman:\n",
      "He was too hard for him directly, to say the troth\n",
      "on't: before Corioli he scotched him and notched\n",
      "him like a carbon ado.\n",
      "\n",
      "Second Servingman:\n",
      "An he had been cannibally given, he might have\n",
      "broiled and eaten him too.\n",
      "\n",
      "First Servingman:\n",
      "But, more of thy news?\n",
      "\n",
      "Third Servingman:\n",
      "Why, he is so made on here within, as if he were son\n",
      "and heir to Mars; set at upper end o' the table; no\n",
      "question asked him by any of the senators, but they\n",
      "stand bald before him: our general himself makes a\n",
      "mistress of him: sanctifies himself with's hand and\n",
      "turns up the white o' the eye to his discourse. But\n",
      "the bottom of the news is that our general is cut i'\n",
      "the middle and but one half of what he was\n",
      "yesterday; for the other has half, by the entreaty\n",
      "and grant of the whole table. He'll go, he says,\n",
      "and sowl the porter of Rome gates by the\n",
      "---------------\n",
      "\n",
      "Her suit is granted for her husband's lands.\n",
      "\n",
      "Nobleman:\n",
      "My gracious lord, Henry your foe is taken,\n",
      "And brought your prisoner to your palace gate.\n",
      "\n",
      "KING EDWARD IV:\n",
      "See that he be convey'd unto the Tower:\n",
      "And go we, brothers, to the man that took him,\n",
      "To question of his apprehension.\n",
      "Widow, go you along. Lords, use her honourably.\n",
      "\n",
      "GLOUCESTER:\n",
      "Ay, Edward will use women honourably.\n",
      "Would he were wasted, marrow, bones and all,\n",
      "That from his loins no hopeful branch may spring,\n",
      "To cross me from the golden time I look for!\n",
      "And yet, between my soul's desire and me--\n",
      "The lustful Edward's title buried--\n",
      "Is Clarence, Henry, and his son young Edward,\n",
      "And all the unlook'd for issue of their bodies,\n",
      "To take their rooms, ere I can place myself:\n",
      "A cold premeditation for my purpose!\n",
      "Why, then, I do but dream on sovereignty;\n",
      "Like one that stands upon a promontory,\n",
      "And spies a far-off shore where he would tread,\n",
      "Wishing his foot were equal with his eye,\n",
      "And chides the sea that sunders him from thence,\n",
      "Saying, he'll lade it dry to have his way:\n",
      "So do I wish the crown, being so far off;\n",
      "And so I chide the means that keeps me from it;\n",
      "And so I say, I'll cut the causes off,\n",
      "Flattering me with impossibilities.\n",
      "My eye's too quick, my heart o'erweens too much,\n",
      "Unless my hand and strength could equal them.\n",
      "Well, say there is no kingdom then for Richard;\n",
      "What other pleasure can the world afford?\n",
      "I'll make my heaven in a lady's lap,\n",
      "And deck my body in gay ornaments,\n",
      "And witch sweet ladies with my words and looks.\n",
      "O miserable thought! and more unlikely\n",
      "Than to accomplish twenty golden crowns!\n",
      "Why, love forswore me in my mother's womb:\n",
      "And, for I should not deal in her soft laws,\n",
      "She did corrupt frail nature with some bribe,\n",
      "To shrink mine arm up like a wither'd shrub;\n",
      "To make an envious mountain on my back,\n",
      "Where sits deformity to\n",
      "---------------\n",
      "\n",
      "Pray, go, let us sup.\n",
      "\n",
      "VOLUMNIA:\n",
      "Let them sup upon myself, and sup upon myself,\n",
      "And so shall starve with feeding. Come, let's go:\n",
      "Leave this faint puling and lament as I do,\n",
      "In anger, Juno-like. Come, come, come.\n",
      "\n",
      "MENENIUS:\n",
      "Fie, fie, fie!\n",
      "\n",
      "Roman:\n",
      "I know you well, sir, and you know\n",
      "me: your name, I think, is Adrian.\n",
      "\n",
      "Volsce:\n",
      "It is so, sir: truly, I have forgot you.\n",
      "\n",
      "Roman:\n",
      "I am a Roman; and my services are,\n",
      "as you are, against 'em: know you me yet?\n",
      "\n",
      "Volsce:\n",
      "Nicanor? no.\n",
      "\n",
      "Roman:\n",
      "The same, sir.\n",
      "\n",
      "Volsce:\n",
      "You had more beard when I last saw you; but your\n",
      "favour is well approved by your tongue. What's the\n",
      "news in Rome? I have a note from the Volscian state,\n",
      "to find you out there: you have well saved me a\n",
      "day's journey.\n",
      "\n",
      "Roman:\n",
      "There hath been in Rome strange insurrections; the\n",
      "people against the senators, patricians, and nobles.\n",
      "\n",
      "Volsce:\n",
      "Hath been! is it ended, then? Our state thinks not\n",
      "so: they are in a most warlike preparation, and\n",
      "hope to come upon them in the heat of their division.\n",
      "\n",
      "Roman:\n",
      "The main blaze of it is past, but a small thing\n",
      "would make it flame again: for the nobles receive\n",
      "so to heart the banishment of that worthy\n",
      "Coriolanus, that they are in a ripe aptness to take\n",
      "all power from the people and to pluck from them\n",
      "their tribunes for ever. This lies glowing, I can\n",
      "tell you, and is almost mature for the violent\n",
      "breaking out.\n",
      "\n",
      "Volsce:\n",
      "Coriolanus banished!\n",
      "\n",
      "Roman:\n",
      "Banished, sir.\n",
      "\n",
      "Volsce:\n",
      "You will be welcome with this intelligence, Nicanor.\n",
      "\n",
      "Roman:\n",
      "The day serves well for them now. I have heard it\n",
      "said, the fittest time to corrupt a man's wife is\n",
      "when she's fallen out with her husband. Your\n",
      "---------------\n",
      "\n",
      "MERCUTIO:\n",
      "A plague o' both your houses! I am sped.\n",
      "Is he gone, and hath nothing?\n",
      "\n",
      "BENVOLIO:\n",
      "What, art thou hurt?\n",
      "\n",
      "MERCUTIO:\n",
      "Ay, ay, a scratch, a scratch; marry, 'tis enough.\n",
      "Where is my page? Go, villain, fetch a surgeon.\n",
      "\n",
      "ROMEO:\n",
      "Courage, man; the hurt cannot be much.\n",
      "\n",
      "MERCUTIO:\n",
      "No, 'tis not so deep as a well, nor so wide as a\n",
      "church-door; but 'tis enough,'twill serve: ask for\n",
      "me to-morrow, and you shall find me a grave man. I\n",
      "am peppered, I warrant, for this world. A plague o'\n",
      "both your houses! 'Zounds, a dog, a rat, a mouse, a\n",
      "cat, to scratch a man to death! a braggart, a\n",
      "rogue, a villain, that fights by the book of\n",
      "arithmetic! Why the devil came you between us? I\n",
      "was hurt under your arm.\n",
      "\n",
      "ROMEO:\n",
      "I thought all for the best.\n",
      "\n",
      "MERCUTIO:\n",
      "Help me into some house, Benvolio,\n",
      "Or I shall faint. A plague o' both your houses!\n",
      "They have made worms' meat of me: I have it,\n",
      "And soundly too: your houses!\n",
      "\n",
      "ROMEO:\n",
      "This gentleman, the prince's near ally,\n",
      "My very friend, hath got his mortal hurt\n",
      "In my behalf; my reputation stain'd\n",
      "With Tybalt's slander,--Tybalt, that an hour\n",
      "Hath been my kinsman! O sweet Juliet,\n",
      "Thy beauty hath made me effeminate\n",
      "And in my temper soften'd valour's steel!\n",
      "\n",
      "BENVOLIO:\n",
      "O Romeo, Romeo, brave Mercutio's dead!\n",
      "That gallant spirit hath aspired the clouds,\n",
      "Which too untimely here did scorn the earth.\n",
      "\n",
      "ROMEO:\n",
      "This day's black fate on more days doth depend;\n",
      "This but begins the woe, others must end.\n",
      "\n",
      "BENVOLIO:\n",
      "Here comes the furious Tybalt back again.\n",
      "\n",
      "ROMEO:\n",
      "Alive, in triumph! and Mercutio slain!\n",
      "Away to heaven, respective lenity,\n",
      "And fire-eyed fury be my conduct now!\n",
      "Now\n",
      "---------------\n",
      "\n",
      "MARIANA:\n",
      "Neither, my lord.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "Why, you are nothing then: neither maid, widow, nor wife?\n",
      "\n",
      "LUCIO:\n",
      "My lord, she may be a punk; for many of them are\n",
      "neither maid, widow, nor wife.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "Silence that fellow: I would he had some cause\n",
      "To prattle for himself.\n",
      "\n",
      "LUCIO:\n",
      "Well, my lord.\n",
      "\n",
      "MARIANA:\n",
      "My lord; I do confess I ne'er was married;\n",
      "And I confess besides I am no maid:\n",
      "I have known my husband; yet my husband\n",
      "Knows not that ever he knew me.\n",
      "\n",
      "LUCIO:\n",
      "He was drunk then, my lord: it can be no better.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "For the benefit of silence, would thou wert so too!\n",
      "\n",
      "LUCIO:\n",
      "Well, my lord.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "This is no witness for Lord Angelo.\n",
      "MARIANA:\n",
      "Now I come to't my lord\n",
      "She that accuses him of fornication,\n",
      "In self-same manner doth accuse my husband,\n",
      "And charges him my lord, with such a time\n",
      "When I'll depose I had him in mine arms\n",
      "With all the effect of love.\n",
      "\n",
      "ANGELO:\n",
      "Charges she more than me?\n",
      "\n",
      "MARIANA:\n",
      "Not that I know.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "No? you say your husband.\n",
      "\n",
      "MARIANA:\n",
      "Why, just, my lord, and that is Angelo,\n",
      "Who thinks he knows that he ne'er knew my body,\n",
      "But knows he thinks that he knows Isabel's.\n",
      "\n",
      "ANGELO:\n",
      "This is a strange abuse. Let's see thy face.\n",
      "\n",
      "MARIANA:\n",
      "My husband bids me; now I will unmask.\n",
      "This is that face, thou cruel Angelo,\n",
      "Which once thou sworest was worth the looking on;\n",
      "This is the hand which, with a vow'd contract,\n",
      "Was fast belock'd in thine; this is the body\n",
      "That took away the match from Isabel,\n",
      "And did supply thee at thy garden-house\n",
      "In her imagined person.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "Know you this woman?\n",
      "---------------\n"
     ]
    }
   ]
  }
 ]
}
